{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.5.0\n",
      "Torchvision Version:  0.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.optim as optim\n",
    "import BatchMaker\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(9, 64,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64))\n",
    "    \n",
    "        self.layer2 = nn.Sequential( \n",
    "            nn.Conv2d(64, 64,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.MaxPool2d(2, stride=2, padding=0))\n",
    "            \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64,128,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128))\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128))\n",
    "            \n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.MaxPool2d(2, stride=2, padding=0))\n",
    "            \n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256))\n",
    "            \n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256))\n",
    "            \n",
    "        self.layer9 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256))\n",
    "            \n",
    "        self.layer10 = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "            \n",
    "        self.layer11 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128))\n",
    "            \n",
    "        self.layer12 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128))\n",
    "            \n",
    "        self.layer13 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128))\n",
    "            \n",
    "        self.layer14 = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "            \n",
    "        self.layer15 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64))\n",
    "            \n",
    "        self.layer16 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64))\n",
    "            \n",
    "        self.layer17 = nn.Sequential(\n",
    "            nn.Conv2d(64, 1,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(1))\n",
    "            \n",
    "            \n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)  \n",
    "        x = self.layer2(x)  \n",
    "        x = self.layer3(x)  \n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)  \n",
    "        x = self.layer7(x)\n",
    "        x = self.layer8(x)\n",
    "        x = self.layer9(x)  \n",
    "        x = self.layer10(x)  \n",
    "        x = self.layer11(x)  \n",
    "        x = self.layer12(x)  \n",
    "        x = self.layer13(x)  \n",
    "        x = self.layer14(x)  \n",
    "        x = self.layer15(x)  \n",
    "        x = self.layer16(x)\n",
    "        x = self.layer17(x) \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on your GeForce GTX 1080 (GPU)\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and move the model over to GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda: 0\")\n",
    "    gpu_name = torch.cuda.get_device_name()\n",
    "    print(f\"Running on your {gpu_name} (GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on your CPU\")\n",
    "\n",
    "net = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generator = BatchMaker.BatchMaker\n",
    "\n",
    "batch_size = 1\n",
    "training_file = \"training.csv\"\n",
    "testing_file = \"testing.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "----------\n",
      "Training Loss: 34086.1683\n",
      "Training Loss: 27450.4150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elias\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:88: UserWarning: Using a target size (torch.Size([1, 180, 240])) that is different to the input size (torch.Size([1, 1, 180, 240])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 15299.3694\n",
      "Training Loss: 9004.5104\n",
      "Training Loss: 8671.5615\n",
      "Training Loss: 8301.8524\n",
      "Training Loss: 10481.1345\n",
      "Training Loss: 6925.1842\n",
      "Training Loss: 5357.2153\n",
      "Training Loss: 5273.2144\n",
      "Training Loss: 4573.9428\n",
      "Training Loss: 4221.2831\n",
      "Training Loss: 3880.0721\n",
      "Training Loss: 3291.6574\n",
      "Training Loss: 3234.1603\n",
      "Training Loss: 3230.5309\n",
      "Training Loss: 3899.7386\n",
      "Training Loss: 2800.9797\n",
      "Training Loss: 5183.7385\n",
      "Training Loss: 2359.2413\n",
      "Training Loss: 2601.3941\n",
      "Training Loss: 2475.3319\n",
      "Training Loss: 4396.9612\n",
      "Training Loss: 4294.7806\n",
      "Training Loss: 2715.8656\n",
      "Training Loss: 1973.0419\n",
      "Training Loss: 2597.4471\n",
      "Training Loss: 3982.0544\n",
      "Training Loss: 3083.3913\n",
      "Training Loss: 2616.2866\n",
      "Training Loss: 3402.7675\n",
      "Training Loss: 2703.0968\n",
      "Training Loss: 1451.6043\n",
      "Training Loss: 1456.2591\n",
      "Training Loss: 1520.7868\n",
      "Training Loss: 1569.3016\n",
      "Training Loss: 7053.5138\n",
      "Training Loss: 2057.8034\n",
      "Training Loss: 2715.5516\n",
      "Training Loss: 1933.2073\n",
      "Training Loss: 2234.7379\n",
      "Training Loss: 2220.7142\n",
      "Training Loss: 4240.5105\n",
      "Training Loss: 2084.5134\n",
      "Training Loss: 1748.2219\n",
      "Training Loss: 2187.5215\n",
      "Training Loss: 1783.1865\n",
      "Training Loss: 4076.5100\n",
      "Training Loss: 1974.0450\n",
      "Training Loss: 1863.4903\n",
      "Training Loss: 1868.5403\n",
      "Training Loss: 2983.4072\n",
      "Training Loss: 1914.1522\n",
      "Training Loss: 2392.3430\n",
      "Training Loss: 1926.7536\n",
      "Training Loss: 1863.7987\n",
      "Training Loss: 2917.6153\n",
      "Training Loss: 1905.3712\n",
      "Training Loss: 1923.2936\n",
      "Training Loss: 1914.7144\n",
      "Training Loss: 1903.1142\n",
      "Training Loss: 1885.5720\n",
      "Training Loss: 1871.5884\n",
      "Training Loss: 1844.0006\n",
      "Training Loss: 1823.0129\n",
      "Training Loss: 1874.6540\n",
      "Training Loss: 1765.3523\n",
      "Training Loss: 1697.7487\n",
      "Training Loss: 2517.0752\n",
      "Training Loss: 1613.8181\n",
      "Training Loss: 1649.4113\n",
      "Training Loss: 1594.3659\n",
      "Training Loss: 1606.0978\n",
      "Training Loss: 1561.5285\n",
      "Training Loss: 1717.9035\n",
      "Training Loss: 1551.6591\n",
      "Training Loss: 1529.2988\n",
      "Training Loss: 2477.3231\n",
      "Training Loss: 1464.6132\n",
      "Training Loss: 1459.9984\n",
      "Training Loss: 1440.6291\n",
      "Training Loss: 2239.5259\n",
      "Training Loss: 1409.8388\n",
      "Training Loss: 1395.1376\n",
      "Training Loss: 5255.7966\n",
      "Training Loss: 1349.8338\n",
      "Training Loss: 1758.8167\n",
      "Training Loss: 1319.5010\n",
      "Training Loss: 1988.3666\n",
      "Training Loss: 1812.6698\n",
      "Training Loss: 2124.5160\n",
      "Training Loss: 8427.8115\n",
      "Training Loss: 3763.1084\n",
      "Training Loss: 3066.4813\n",
      "Training Loss: 1732.7579\n",
      "Training Loss: 2568.4596\n",
      "Training Loss: 1577.2273\n",
      "Training Loss: 1432.4693\n",
      "Training Loss: 1380.6893\n",
      "Training Loss: 1367.0431\n",
      "Training Loss: 2017.8751\n",
      "Training Loss: 1404.9945\n",
      "Training Loss: 1444.3749\n",
      "Training Loss: 1827.8529\n",
      "Training Loss: 1453.0093\n",
      "Training Loss: 1454.5384\n",
      "Training Loss: 1451.9979\n",
      "Training Loss: 1387.1811\n",
      "Training Loss: 3662.8590\n",
      "Training Loss: 1422.0385\n",
      "Training Loss: 2097.1304\n",
      "Training Loss: 1363.6132\n",
      "Training Loss: 1640.7770\n",
      "Training Loss: 2977.3093\n",
      "Training Loss: 1390.2510\n",
      "Training Loss: 1397.6628\n",
      "Training Loss: 1988.4060\n",
      "Training Loss: 1386.9963\n",
      "Training Loss: 1366.2536\n",
      "Training Loss: 1353.7843\n",
      "Training Loss: 1355.2204\n",
      "Training Loss: 1336.3227\n",
      "Training Loss: 1327.3294\n",
      "Training Loss: 1312.6521\n",
      "Training Loss: 1291.1612\n",
      "Training Loss: 1277.1980\n",
      "Training Loss: 1240.4804\n",
      "Training Loss: 1228.6211\n",
      "Training Loss: 1196.6355\n",
      "Training Loss: 1157.5922\n",
      "Training Loss: 1131.8615\n",
      "Training Loss: 1101.5389\n",
      "Training Loss: 1064.3690\n",
      "Training Loss: 2162.2148\n",
      "Training Loss: 1019.6807\n",
      "Training Loss: 997.4323\n",
      "Training Loss: 972.0436\n",
      "Training Loss: 933.1463\n",
      "Training Loss: 1665.9178\n",
      "Training Loss: 894.7648\n",
      "Training Loss: 958.4791\n",
      "Training Loss: 4040.1852\n",
      "Training Loss: 1526.6975\n",
      "Training Loss: 2822.0958\n",
      "Training Loss: 846.6201\n",
      "Training Loss: 1655.2220\n",
      "Training Loss: 838.6851\n",
      "Training Loss: 1372.2031\n",
      "Training Loss: 4301.1981\n",
      "Training Loss: 855.7528\n",
      "Training Loss: 854.7417\n",
      "Training Loss: 853.0673\n",
      "Training Loss: 905.4990\n",
      "Training Loss: 843.0699\n",
      "Training Loss: 837.5586\n",
      "Training Loss: 828.4107\n",
      "Training Loss: 815.5622\n",
      "Training Loss: 799.3878\n",
      "Training Loss: 780.2245\n",
      "Training Loss: 758.3751\n",
      "Training Loss: 734.1121\n",
      "Training Loss: 707.7014\n",
      "Training Loss: 679.3520\n",
      "Training Loss: 649.2575\n",
      "Training Loss: 617.5955\n",
      "Training Loss: 584.5256\n",
      "Training Loss: 550.2010\n",
      "Training Loss: 514.7615\n",
      "Training Loss: 478.3067\n",
      "Training Loss: 440.9404\n",
      "Training Loss: 402.7563\n",
      "Training Loss: 363.8622\n",
      "Training Loss: 324.3189\n",
      "Training Loss: 284.1888\n",
      "Training Loss: 243.5326\n",
      "Training Loss: 202.4734\n",
      "Training Loss: 161.0437\n",
      "Training Loss: 119.2468\n",
      "Training Loss: 77.1212\n",
      "Training Loss: 89.3751\n",
      "Training Loss: 123.4905\n",
      "Training Loss: 149.5119\n",
      "Training Loss: 168.2547\n",
      "Training Loss: 180.4527\n",
      "Training Loss: 186.7654\n",
      "Training Loss: 187.7863\n",
      "Training Loss: 184.0492\n",
      "Training Loss: 176.0340\n",
      "Training Loss: 164.1726\n",
      "Training Loss: 148.8532\n",
      "Training Loss: 130.4252\n",
      "Training Loss: 109.2026\n",
      "Training Loss: 85.4682\n",
      "Training Loss: 64.1177\n",
      "Training Loss: 82.6347\n",
      "Training Loss: 94.8057\n",
      "Training Loss: 101.2684\n",
      "Training Loss: 102.5970\n",
      "Training Loss: 99.3076\n",
      "Training Loss: 91.8649\n",
      "Training Loss: 80.6867\n",
      "Training Loss: 66.1489\n",
      "Training Loss: 75.2588\n",
      "Training Loss: 86.6895\n",
      "Training Loss: 92.3493\n",
      "Training Loss: 92.8193\n",
      "Training Loss: 88.6219\n",
      "Training Loss: 80.2273\n",
      "Training Loss: 68.0585\n",
      "Training Loss: 70.9850\n",
      "Training Loss: 80.2956\n",
      "Training Loss: 84.2084\n",
      "Training Loss: 83.2657\n",
      "Training Loss: 77.9553\n",
      "Training Loss: 68.7163\n",
      "Training Loss: 67.7841\n",
      "Training Loss: 74.8583\n",
      "Training Loss: 76.6187\n",
      "Training Loss: 73.5999\n",
      "Training Loss: 66.2829\n",
      "Training Loss: 68.4225\n",
      "Training Loss: 73.8708\n",
      "Training Loss: 74.3232\n",
      "Training Loss: 70.2815\n",
      "Training Loss: 62.1972\n",
      "Training Loss: 73.3413\n",
      "Training Loss: 79.4675\n",
      "Training Loss: 80.3883\n",
      "Training Loss: 76.6272\n",
      "Training Loss: 68.6552\n",
      "Training Loss: 66.6558\n",
      "Training Loss: 72.6271\n",
      "Training Loss: 73.5625\n",
      "Training Loss: 69.9674\n",
      "Training Loss: 62.2967\n",
      "Training Loss: 72.8498\n",
      "Training Loss: 78.6373\n",
      "Training Loss: 197.7405\n",
      "Training Loss: 75.2625\n",
      "Training Loss: 67.0845\n",
      "Training Loss: 68.3724\n",
      "Training Loss: 221.0337\n",
      "Training Loss: 84.5280\n",
      "Training Loss: 921.8871\n",
      "Training Loss: 97.7420\n",
      "Training Loss: 101.0789\n",
      "Training Loss: 99.6590\n",
      "Training Loss: 93.9596\n",
      "Training Loss: 84.4103\n",
      "Training Loss: 71.3975\n",
      "Training Loss: 68.4699\n",
      "Training Loss: 78.6558\n",
      "Training Loss: 83.2575\n",
      "Training Loss: 82.8360\n",
      "Training Loss: 77.8959\n",
      "Training Loss: 68.8913\n",
      "Training Loss: 67.3104\n",
      "Training Loss: 74.1072\n",
      "Training Loss: 75.8125\n",
      "Training Loss: 72.9368\n",
      "Training Loss: 65.9398\n",
      "Training Loss: 333.8579\n",
      "Training Loss: 73.7547\n",
      "Training Loss: 73.9262\n",
      "Training Loss: 69.5290\n",
      "Training Loss: 62.5964\n",
      "Training Loss: 65.7239\n",
      "Training Loss: 64.1351\n",
      "Training Loss: 65.3861\n",
      "Training Loss: 66.1729\n",
      "Training Loss: 62.3346\n",
      "Training Loss: 69.1746\n",
      "Training Loss: 71.8574\n",
      "Training Loss: 69.8734\n",
      "Training Loss: 63.6905\n",
      "Training Loss: 70.0342\n",
      "Training Loss: 74.6024\n",
      "Training Loss: 96.3377\n",
      "Training Loss: 69.2465\n",
      "Training Loss: 63.3310\n",
      "Training Loss: 66.8809\n",
      "Training Loss: 65.6836\n",
      "Training Loss: 63.4426\n",
      "Training Loss: 63.9085\n",
      "Training Loss: 63.8057\n",
      "Training Loss: 63.0606\n",
      "Training Loss: 65.6922\n",
      "Training Loss: 65.7863\n",
      "Training Loss: 62.2844\n",
      "Training Loss: 61.8357\n",
      "Training Loss: 66.6631\n",
      "Training Loss: 66.5136\n",
      "Training Loss: 61.8503\n",
      "Training Loss: 70.3646\n",
      "Training Loss: 73.7062\n",
      "Training Loss: 72.3316\n",
      "Training Loss: 66.7135\n",
      "Training Loss: 66.4286\n",
      "Training Loss: 70.5342\n",
      "Training Loss: 69.7047\n",
      "Training Loss: 64.4352\n",
      "Training Loss: 68.3528\n",
      "Training Loss: 72.1778\n",
      "Training Loss: 71.2437\n",
      "Training Loss: 66.0274\n",
      "Training Loss: 66.7532\n",
      "Training Loss: 70.5289\n",
      "Training Loss: 69.4079\n",
      "Training Loss: 63.8815\n",
      "Training Loss: 69.1198\n",
      "Training Loss: 73.1501\n",
      "Training Loss: 72.4058\n",
      "Training Loss: 67.3652\n",
      "Training Loss: 65.2276\n",
      "Training Loss: 68.8592\n",
      "Training Loss: 67.6138\n",
      "Training Loss: 61.9803\n",
      "Training Loss: 71.0800\n",
      "Training Loss: 75.1960\n",
      "Training Loss: 74.5336\n",
      "Training Loss: 69.5716\n",
      "Training Loss: 62.9084\n",
      "Training Loss: 66.4761\n",
      "Training Loss: 65.1778\n",
      "Training Loss: 64.0923\n",
      "Training Loss: 64.7548\n",
      "Training Loss: 62.6566\n",
      "Training Loss: 62.0327\n",
      "Training Loss: 65.0730\n",
      "Training Loss: 63.6974\n",
      "Training Loss: 65.6139\n",
      "Training Loss: 66.3384\n",
      "Training Loss: 62.6310\n",
      "Training Loss: 68.8086\n",
      "Training Loss: 71.3444\n",
      "Training Loss: 69.1244\n",
      "Training Loss: 62.6255\n",
      "Training Loss: 71.2003\n",
      "Training Loss: 76.0069\n",
      "Training Loss: 75.9770\n",
      "Training Loss: 71.5949\n",
      "Training Loss: 63.2967\n",
      "Training Loss: 72.3266\n",
      "Training Loss: 78.6418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 79.8274\n",
      "Training Loss: 76.3976\n",
      "Training Loss: 68.8154\n",
      "Training Loss: 66.0646\n",
      "Training Loss: 71.7349\n",
      "Training Loss: 72.4872\n",
      "Training Loss: 68.8141\n",
      "Training Loss: 62.4835\n",
      "Training Loss: 64.9922\n",
      "Training Loss: 62.7572\n",
      "Training Loss: 67.2876\n",
      "Training Loss: 68.6978\n",
      "Training Loss: 65.6196\n",
      "Training Loss: 65.1834\n",
      "Training Loss: 67.2029\n",
      "Training Loss: 64.5304\n",
      "Training Loss: 65.9276\n",
      "Training Loss: 67.6866\n",
      "Training Loss: 64.9250\n",
      "Training Loss: 65.5971\n",
      "Training Loss: 67.3562\n",
      "Training Loss: 64.4523\n",
      "Training Loss: 66.2067\n",
      "Training Loss: 68.1504\n",
      "Training Loss: 65.5576\n",
      "Training Loss: 64.7970\n",
      "Training Loss: 66.4176\n",
      "Training Loss: 63.3915\n",
      "Training Loss: 67.3558\n",
      "Training Loss: 69.3970\n",
      "Training Loss: 66.8944\n",
      "Training Loss: 63.3531\n",
      "Training Loss: 64.8996\n",
      "Training Loss: 61.8217\n",
      "Training Loss: 63.4377\n",
      "Training Loss: 63.1869\n",
      "Training Loss: 62.1162\n",
      "Training Loss: 64.4277\n",
      "Training Loss: 62.7082\n",
      "Training Loss: 66.8884\n",
      "Training Loss: 67.7908\n",
      "Training Loss: 64.1243\n",
      "Training Loss: 67.1961\n",
      "Training Loss: 69.7481\n",
      "Training Loss: 67.7108\n",
      "Training Loss: 62.0921\n",
      "Training Loss: 63.2563\n",
      "Training Loss: 63.7714\n",
      "Training Loss: 62.4746\n",
      "Training Loss: 66.7354\n",
      "Training Loss: 67.2904\n",
      "Training Loss: 63.3152\n",
      "Training Loss: 68.2617\n",
      "Training Loss: 71.0600\n",
      "Training Loss: 69.2482\n",
      "Training Loss: 63.2878\n",
      "Training Loss: 70.1724\n",
      "Training Loss: 74.5665\n",
      "Training Loss: 74.0487\n",
      "Training Loss: 69.1112\n",
      "Training Loss: 63.4084\n",
      "Training Loss: 66.9737\n",
      "Training Loss: 65.8550\n",
      "Training Loss: 63.1314\n",
      "Training Loss: 63.5403\n",
      "Training Loss: 64.1541\n",
      "Training Loss: 63.4597\n",
      "Training Loss: 65.1766\n",
      "Training Loss: 65.2364\n",
      "Training Loss: 62.7931\n",
      "Training Loss: 62.3769\n",
      "Training Loss: 66.0214\n",
      "Training Loss: 65.8523\n",
      "Training Loss: 62.3884\n",
      "Training Loss: 62.1547\n",
      "Training Loss: 66.0789\n",
      "Training Loss: 65.7598\n",
      "Training Loss: 62.6110\n",
      "Training Loss: 62.4970\n",
      "Training Loss: 65.6202\n",
      "Training Loss: 65.2027\n",
      "Training Loss: 63.2448\n",
      "Training Loss: 63.2093\n",
      "Training Loss: 64.8232\n",
      "Training Loss: 64.3412\n",
      "Training Loss: 64.1482\n",
      "Training Loss: 64.1641\n",
      "Training Loss: 63.8043\n",
      "Training Loss: 63.2801\n",
      "Training Loss: 65.2283\n",
      "Training Loss: 65.2780\n",
      "Training Loss: 62.6400\n",
      "Training Loss: 62.0882\n",
      "Training Loss: 66.4242\n",
      "Training Loss: 66.4960\n",
      "Training Loss: 62.2438\n",
      "Training Loss: 69.6577\n",
      "Training Loss: 72.6473\n",
      "Training Loss: 70.8790\n",
      "Training Loss: 64.8293\n",
      "Training Loss: 68.5929\n",
      "Training Loss: 73.0457\n",
      "Training Loss: 72.7383\n",
      "Training Loss: 68.1471\n",
      "Training Loss: 63.9651\n",
      "Training Loss: 67.2340\n",
      "Training Loss: 65.7194\n",
      "Training Loss: 63.7001\n",
      "Training Loss: 64.5394\n",
      "Training Loss: 62.6631\n",
      "Training Loss: 62.1638\n",
      "Training Loss: 64.7639\n",
      "Training Loss: 63.2808\n",
      "Training Loss: 66.0697\n",
      "Training Loss: 66.8841\n",
      "Training Loss: 63.3058\n",
      "Training Loss: 67.9561\n",
      "Training Loss: 70.3916\n",
      "Training Loss: 68.1304\n",
      "Training Loss: 61.9857\n",
      "Training Loss: 63.4202\n",
      "Training Loss: 63.2530\n",
      "Training Loss: 62.0651\n",
      "Training Loss: 64.4203\n",
      "Training Loss: 62.5411\n",
      "Training Loss: 67.1450\n",
      "Training Loss: 68.2753\n",
      "Training Loss: 64.9845\n",
      "Training Loss: 65.9838\n",
      "Training Loss: 68.1830\n",
      "Training Loss: 65.7124\n",
      "Training Loss: 64.5469\n",
      "Training Loss: 66.1485\n",
      "Training Loss: 63.2834\n",
      "Training Loss: 67.3218\n",
      "Training Loss: 69.1708\n",
      "Training Loss: 66.3866\n",
      "Training Loss: 64.1599\n",
      "Training Loss: 66.0116\n",
      "Training Loss: 63.3730\n",
      "Training Loss: 67.0221\n",
      "Training Loss: 68.6848\n",
      "Training Loss: 65.7344\n",
      "Training Loss: 64.9471\n",
      "Training Loss: 66.9314\n",
      "Training Loss: 64.4135\n",
      "Training Loss: 65.8527\n",
      "Training Loss: 67.4162\n",
      "Training Loss: 64.3780\n",
      "Training Loss: 66.3580\n",
      "Training Loss: 68.4123\n",
      "Training Loss: 65.9589\n",
      "Training Loss: 64.2215\n",
      "Training Loss: 65.7321\n",
      "Training Loss: 62.6477\n",
      "Training Loss: 68.0998\n",
      "Training Loss: 70.1910\n",
      "Training Loss: 67.7722\n",
      "Training Loss: 62.3454\n",
      "Training Loss: 63.8277\n",
      "Training Loss: 62.8945\n",
      "Training Loss: 62.2922\n",
      "Training Loss: 64.3244\n",
      "Training Loss: 62.7046\n",
      "Training Loss: 66.7640\n",
      "Training Loss: 67.5888\n",
      "Training Loss: 63.8898\n",
      "Training Loss: 67.4192\n",
      "Training Loss: 70.0003\n",
      "Training Loss: 68.0250\n",
      "Training Loss: 61.9492\n",
      "Training Loss: 71.6065\n",
      "Training Loss: 76.0998\n",
      "Training Loss: 75.7036\n",
      "Training Loss: 70.9077\n",
      "Training Loss: 62.1526\n",
      "Training Loss: 73.6031\n",
      "Training Loss: 80.2133\n",
      "Training Loss: 81.8661\n",
      "Training Loss: 79.0574\n",
      "Training Loss: 72.2335\n",
      "Training Loss: 61.8354\n",
      "Training Loss: 66.9446\n",
      "Training Loss: 67.1050\n",
      "Training Loss: 62.8120\n",
      "Training Loss: 69.0018\n",
      "Training Loss: 72.0566\n",
      "Training Loss: 70.5113\n",
      "Training Loss: 64.8261\n",
      "Training Loss: 68.3213\n",
      "Training Loss: 72.4934\n",
      "Training Loss: 71.8121\n",
      "Training Loss: 66.7632\n",
      "Training Loss: 65.7821\n",
      "Training Loss: 69.4394\n",
      "Training Loss: 68.4379\n",
      "Training Loss: 63.2436\n",
      "Training Loss: 69.4791\n",
      "Training Loss: 73.2473\n",
      "Training Loss: 72.2040\n",
      "Training Loss: 66.8309\n",
      "Training Loss: 66.0009\n",
      "Training Loss: 69.9167\n",
      "Training Loss: 69.1492\n",
      "Training Loss: 64.1672\n",
      "Training Loss: 68.3448\n",
      "Training Loss: 71.9384\n",
      "Training Loss: 70.7396\n",
      "Training Loss: 65.2282\n",
      "Training Loss: 67.6988\n",
      "Training Loss: 71.7250\n",
      "Training Loss: 71.0584\n",
      "Training Loss: 66.1686\n",
      "Training Loss: 66.2247\n",
      "Training Loss: 69.7426\n",
      "Training Loss: 68.4771\n",
      "Training Loss: 62.9072\n",
      "Training Loss: 70.0327\n",
      "Training Loss: 74.1057\n",
      "Training Loss: 73.4826\n",
      "Training Loss: 68.6333\n",
      "Training Loss: 63.6810\n",
      "Training Loss: 67.1657\n",
      "Training Loss: 65.8717\n",
      "Training Loss: 63.3289\n",
      "Training Loss: 63.9946\n",
      "Training Loss: 63.3500\n",
      "Training Loss: 62.2947\n",
      "Training Loss: 66.6362\n",
      "Training Loss: 67.1119\n",
      "Training Loss: 63.2531\n",
      "Training Loss: 68.2417\n",
      "Training Loss: 70.9112\n",
      "Training Loss: 68.8854\n",
      "Training Loss: 62.6343\n",
      "Training Loss: 70.9012\n",
      "Training Loss: 75.5171\n",
      "Training Loss: 75.3858\n",
      "Training Loss: 70.9821\n",
      "Training Loss: 62.7334\n",
      "Training Loss: 72.7842\n",
      "Training Loss: 79.0678\n",
      "Training Loss: 80.2959\n",
      "Training Loss: 76.9747\n",
      "Training Loss: 69.5595\n",
      "Training Loss: 65.1170\n",
      "Training Loss: 70.6608\n",
      "Training Loss: 71.3661\n",
      "Training Loss: 67.7169\n",
      "Training Loss: 63.5098\n",
      "Training Loss: 66.0071\n",
      "Training Loss: 63.8293\n",
      "Training Loss: 66.1164\n",
      "Training Loss: 67.4868\n",
      "Training Loss: 64.4372\n",
      "Training Loss: 66.2941\n",
      "Training Loss: 68.2979\n",
      "Training Loss: 65.6770\n",
      "Training Loss: 64.6899\n",
      "Training Loss: 66.4135\n",
      "Training Loss: 63.6826\n",
      "Training Loss: 66.7685\n",
      "Training Loss: 68.5100\n",
      "Training Loss: 65.6539\n",
      "Training Loss: 64.9199\n",
      "Training Loss: 66.8309\n",
      "Training Loss: 64.2696\n",
      "Training Loss: 66.0158\n",
      "Training Loss: 67.6178\n",
      "Training Loss: 64.6371\n",
      "Training Loss: 66.0297\n",
      "Training Loss: 68.0402\n",
      "Training Loss: 65.5692\n",
      "Training Loss: 64.6114\n",
      "Training Loss: 66.1391\n",
      "Training Loss: 63.0925\n",
      "Training Loss: 67.6069\n",
      "Training Loss: 69.6700\n",
      "Training Loss: 67.2472\n",
      "Training Loss: 62.8608\n",
      "Training Loss: 64.3490\n",
      "Training Loss: 62.3549\n",
      "Training Loss: 62.8435\n",
      "Training Loss: 63.7580\n",
      "Training Loss: 62.1376\n",
      "Training Loss: 67.3196\n",
      "Training Loss: 68.1473\n",
      "Training Loss: 64.4725\n",
      "Training Loss: 66.8037\n",
      "Training Loss: 69.3675\n",
      "Training Loss: 67.3971\n",
      "Training Loss: 62.2930\n",
      "Training Loss: 63.4090\n",
      "Training Loss: 63.6076\n",
      "Training Loss: 62.3539\n",
      "Training Loss: 66.7630\n",
      "Training Loss: 67.2891\n",
      "Training Loss: 63.3441\n",
      "Training Loss: 68.1517\n",
      "Training Loss: 70.9309\n",
      "Training Loss: 69.1556\n",
      "Training Loss: 63.2816\n",
      "Training Loss: 70.0457\n",
      "Training Loss: 74.3756\n",
      "Training Loss: 73.8547\n",
      "Training Loss: 68.9687\n",
      "Training Loss: 63.4502\n",
      "Training Loss: 66.9794\n",
      "Training Loss: 65.8802\n",
      "Training Loss: 63.0355\n",
      "Training Loss: 63.4345\n",
      "Training Loss: 64.2149\n",
      "Training Loss: 63.5326\n",
      "Training Loss: 65.0397\n",
      "Training Loss: 65.0955\n",
      "Training Loss: 62.8840\n",
      "Training Loss: 62.4753\n",
      "Training Loss: 65.8637\n",
      "Training Loss: 65.6945\n",
      "Training Loss: 62.4936\n",
      "Training Loss: 62.2643\n",
      "Training Loss: 65.9136\n",
      "Training Loss: 65.5967\n",
      "Training Loss: 62.7201\n",
      "Training Loss: 62.6084\n",
      "Training Loss: 65.4556\n",
      "Training Loss: 65.0418\n",
      "Training Loss: 63.3512\n",
      "Training Loss: 63.3168\n",
      "Training Loss: 64.6645\n",
      "Training Loss: 64.1873\n",
      "Training Loss: 64.2477\n",
      "Training Loss: 64.2639\n",
      "Training Loss: 63.6550\n",
      "Training Loss: 63.1361\n",
      "Training Loss: 65.3183\n",
      "Training Loss: 65.3677\n",
      "Training Loss: 62.5022\n",
      "Training Loss: 61.9560\n",
      "Training Loss: 66.5030\n",
      "Training Loss: 66.5742\n",
      "Training Loss: 62.3669\n",
      "Training Loss: 69.4458\n",
      "Training Loss: 72.4041\n",
      "Training Loss: 70.6540\n",
      "Training Loss: 64.6667\n",
      "Training Loss: 68.6529\n",
      "Training Loss: 73.0602\n",
      "Training Loss: 72.7560\n",
      "Training Loss: 68.2117\n",
      "Training Loss: 63.8123\n",
      "Training Loss: 67.0481\n",
      "Training Loss: 65.5487\n",
      "Training Loss: 63.8108\n",
      "Training Loss: 64.6418\n",
      "Training Loss: 62.5231\n",
      "Training Loss: 62.2899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 64.6031\n",
      "Training Loss: 63.1345\n",
      "Training Loss: 66.1580\n",
      "Training Loss: 66.9644\n",
      "Training Loss: 63.4209\n",
      "Training Loss: 67.7648\n",
      "Training Loss: 70.1769\n",
      "Training Loss: 67.9374\n",
      "Training Loss: 62.1146\n",
      "Training Loss: 63.5354\n",
      "Training Loss: 63.1067\n",
      "Training Loss: 62.1933\n",
      "Training Loss: 64.2630\n",
      "Training Loss: 62.4014\n",
      "Training Loss: 67.2258\n",
      "Training Loss: 68.3456\n",
      "Training Loss: 65.0854\n",
      "Training Loss: 65.8127\n",
      "Training Loss: 67.9916\n",
      "Training Loss: 65.5437\n",
      "Training Loss: 64.6528\n",
      "Training Loss: 66.2398\n",
      "Training Loss: 63.4008\n",
      "Training Loss: 67.1391\n",
      "Training Loss: 68.9714\n",
      "Training Loss: 66.2122\n",
      "Training Loss: 64.2702\n",
      "Training Loss: 66.1054\n",
      "Training Loss: 63.4903\n",
      "Training Loss: 66.8426\n",
      "Training Loss: 68.4907\n",
      "Training Loss: 65.5662\n",
      "Training Loss: 65.0514\n",
      "Training Loss: 67.0183\n",
      "Training Loss: 64.5224\n",
      "Training Loss: 65.6840\n",
      "Training Loss: 67.2340\n",
      "Training Loss: 64.2219\n",
      "Training Loss: 66.4509\n",
      "Training Loss: 68.4877\n",
      "Training Loss: 66.0552\n",
      "Training Loss: 64.0674\n",
      "Training Loss: 65.5652\n",
      "Training Loss: 62.5068\n",
      "Training Loss: 68.1788\n",
      "Training Loss: 70.2525\n",
      "Training Loss: 67.8539\n",
      "Training Loss: 62.2075\n",
      "Training Loss: 63.6776\n",
      "Training Loss: 63.0169\n",
      "Training Loss: 62.1548\n",
      "Training Loss: 64.4352\n",
      "Training Loss: 62.8286\n",
      "Training Loss: 66.5902\n",
      "Training Loss: 67.4083\n",
      "Training Loss: 63.7392\n",
      "Training Loss: 67.5058\n",
      "Training Loss: 70.0662\n",
      "Training Loss: 68.1067\n",
      "Training Loss: 62.0794\n",
      "Training Loss: 71.3951\n",
      "Training Loss: 75.8527\n",
      "Training Loss: 75.4597\n",
      "Training Loss: 70.7016\n",
      "Training Loss: 62.0150\n",
      "Training Loss: 73.6438\n",
      "Training Loss: 80.2027\n",
      "Training Loss: 81.8426\n",
      "Training Loss: 79.0556\n",
      "Training Loss: 72.2845\n",
      "Training Loss: 61.9276\n",
      "Training Loss: 75.5091\n",
      "Training Loss: 83.5311\n",
      "Training Loss: 86.3470\n",
      "Training Loss: 84.4777\n",
      "Training Loss: 78.3920\n",
      "Training Loss: 68.5118\n",
      "Training Loss: 68.3078\n",
      "Training Loss: 75.8179\n",
      "Training Loss: 78.3151\n",
      "Training Loss: 76.3006\n",
      "Training Loss: 70.2258\n",
      "Training Loss: 63.1563\n",
      "Training Loss: 67.6532\n",
      "Training Loss: 67.2977\n",
      "Training Loss: 62.5752\n",
      "Training Loss: 69.5811\n",
      "Training Loss: 72.9812\n",
      "Training Loss: 71.7801\n",
      "Training Loss: 66.4380\n",
      "Training Loss: 66.3354\n",
      "Training Loss: 70.2292\n",
      "Training Loss: 69.3316\n",
      "Training Loss: 64.1219\n",
      "Training Loss: 68.4900\n",
      "Training Loss: 72.2784\n",
      "Training Loss: 71.4274\n",
      "Training Loss: 66.4010\n",
      "Training Loss: 66.0835\n",
      "Training Loss: 69.7174\n",
      "Training Loss: 68.5865\n",
      "Training Loss: 63.1677\n",
      "Training Loss: 69.6133\n",
      "Training Loss: 73.5685\n",
      "Training Loss: 72.8682\n",
      "Training Loss: 67.9782\n",
      "Training Loss: 64.3550\n",
      "Training Loss: 67.8768\n",
      "Training Loss: 66.6457\n",
      "Training Loss: 62.4831\n",
      "Training Loss: 63.1012\n",
      "Training Loss: 64.2730\n",
      "Training Loss: 63.2601\n",
      "Training Loss: 65.6204\n",
      "Training Loss: 66.0645\n",
      "Training Loss: 62.2053\n",
      "Training Loss: 69.2790\n",
      "Training Loss: 71.9523\n",
      "Training Loss: 69.9584\n",
      "Training Loss: 63.7644\n",
      "Training Loss: 69.7113\n",
      "Training Loss: 74.2843\n",
      "Training Loss: 74.1416\n",
      "Training Loss: 69.7548\n",
      "Training Loss: 62.0875\n",
      "Training Loss: 65.1951\n",
      "Training Loss: 63.5928\n",
      "Training Loss: 65.8139\n",
      "Training Loss: 66.7279\n",
      "Training Loss: 63.2926\n",
      "Training Loss: 67.7850\n",
      "Training Loss: 70.1093\n",
      "Training Loss: 67.8026\n",
      "Training Loss: 62.2954\n",
      "Training Loss: 63.7708\n",
      "Training Loss: 62.8062\n",
      "Training Loss: 62.5319\n",
      "Training Loss: 63.8683\n",
      "Training Loss: 61.9727\n",
      "Training Loss: 67.6667\n",
      "Training Loss: 68.8145\n",
      "Training Loss: 65.5906\n",
      "Training Loss: 65.2550\n",
      "Training Loss: 67.4054\n",
      "Training Loss: 64.9431\n",
      "Training Loss: 65.2456\n",
      "Training Loss: 66.8450\n",
      "Training Loss: 64.0279\n",
      "Training Loss: 66.4706\n",
      "Training Loss: 68.2861\n",
      "Training Loss: 65.5228\n",
      "Training Loss: 64.9413\n",
      "Training Loss: 66.7806\n",
      "Training Loss: 64.1799\n",
      "Training Loss: 66.1178\n",
      "Training Loss: 67.7553\n",
      "Training Loss: 64.8322\n",
      "Training Loss: 65.7617\n",
      "Training Loss: 67.7284\n",
      "Training Loss: 65.2428\n",
      "Training Loss: 64.9318\n",
      "Training Loss: 66.4746\n",
      "Training Loss: 63.4667\n",
      "Training Loss: 67.1798\n",
      "Training Loss: 69.2141\n",
      "Training Loss: 66.7896\n",
      "Training Loss: 63.3030\n",
      "Training Loss: 64.7955\n",
      "Training Loss: 61.8875\n",
      "Training Loss: 63.3201\n",
      "Training Loss: 63.2637\n",
      "Training Loss: 61.9902\n",
      "Training Loss: 64.5111\n",
      "Training Loss: 62.8366\n",
      "Training Loss: 66.6343\n",
      "Training Loss: 67.5094\n",
      "Training Loss: 63.9015\n",
      "Training Loss: 67.2823\n",
      "Training Loss: 69.7948\n",
      "Training Loss: 67.8017\n",
      "Training Loss: 61.8788\n",
      "Training Loss: 63.0163\n",
      "Training Loss: 63.9513\n",
      "Training Loss: 62.6823\n",
      "Training Loss: 66.4195\n",
      "Training Loss: 66.9609\n",
      "Training Loss: 63.0533\n",
      "Training Loss: 68.3814\n",
      "Training Loss: 71.1328\n",
      "Training Loss: 69.3553\n",
      "Training Loss: 63.5019\n",
      "Training Loss: 69.7798\n",
      "Training Loss: 74.0954\n",
      "Training Loss: 73.5851\n",
      "Training Loss: 68.7317\n",
      "Training Loss: 63.6318\n",
      "Training Loss: 67.1370\n",
      "Training Loss: 66.0385\n",
      "Training Loss: 62.8514\n",
      "Training Loss: 63.2525\n",
      "Training Loss: 64.3697\n",
      "Training Loss: 63.6877\n",
      "Training Loss: 64.8596\n",
      "Training Loss: 64.9179\n",
      "Training Loss: 63.0345\n",
      "Training Loss: 62.6257\n",
      "Training Loss: 65.6889\n",
      "Training Loss: 65.5223\n",
      "Training Loss: 62.6388\n",
      "Training Loss: 62.4092\n",
      "Training Loss: 65.7447\n",
      "Training Loss: 65.4305\n",
      "Training Loss: 62.8594\n",
      "Training Loss: 62.7474\n",
      "Training Loss: 65.2931\n",
      "Training Loss: 64.8822\n",
      "Training Loss: 63.4843\n",
      "Training Loss: 63.4495\n",
      "Training Loss: 64.5087\n",
      "Training Loss: 64.0343\n",
      "Training Loss: 64.3745\n",
      "Training Loss: 64.3902\n",
      "Training Loss: 63.5059\n",
      "Training Loss: 62.9898\n",
      "Training Loss: 65.4387\n",
      "Training Loss: 65.4876\n",
      "Training Loss: 62.3599\n",
      "Training Loss: 61.8165\n",
      "Training Loss: 66.6169\n",
      "Training Loss: 66.6877\n",
      "Training Loss: 62.5002\n",
      "Training Loss: 69.2716\n",
      "Training Loss: 72.2160\n",
      "Training Loss: 70.4743\n",
      "Training Loss: 64.5152\n",
      "Training Loss: 68.7567\n",
      "Training Loss: 73.1433\n",
      "Training Loss: 72.8404\n",
      "Training Loss: 68.3172\n",
      "Training Loss: 63.6656\n",
      "Training Loss: 66.8865\n",
      "Training Loss: 65.3940\n",
      "Training Loss: 63.9368\n",
      "Training Loss: 64.7640\n",
      "Training Loss: 62.3825\n",
      "Training Loss: 62.4228\n",
      "Training Loss: 64.4531\n",
      "Training Loss: 62.9912\n",
      "Training Loss: 66.2734\n",
      "Training Loss: 67.0761\n",
      "Training Loss: 63.5487\n",
      "Training Loss: 67.6009\n",
      "Training Loss: 70.0021\n",
      "Training Loss: 67.7727\n",
      "Training Loss: 62.2484\n",
      "Training Loss: 63.6629\n",
      "Training Loss: 62.9636\n",
      "Training Loss: 62.3268\n",
      "Training Loss: 64.1149\n",
      "Training Loss: 62.2615\n",
      "Training Loss: 67.3373\n",
      "Training Loss: 68.4521\n",
      "Training Loss: 65.2061\n",
      "Training Loss: 65.6580\n",
      "Training Loss: 67.8275\n",
      "Training Loss: 65.3902\n",
      "Training Loss: 64.7758\n",
      "Training Loss: 66.3559\n",
      "Training Loss: 63.5291\n",
      "Training Loss: 66.9789\n",
      "Training Loss: 68.8033\n",
      "Training Loss: 66.0559\n",
      "Training Loss: 64.3951\n",
      "Training Loss: 66.2225\n",
      "Training Loss: 63.6185\n",
      "Training Loss: 66.6839\n",
      "Training Loss: 68.3249\n",
      "Training Loss: 65.4128\n",
      "Training Loss: 65.1734\n",
      "Training Loss: 67.1320\n",
      "Training Loss: 64.6465\n",
      "Training Loss: 65.5303\n",
      "Training Loss: 67.0738\n",
      "Training Loss: 64.0742\n",
      "Training Loss: 66.5674\n",
      "Training Loss: 68.5957\n",
      "Training Loss: 66.1732\n",
      "Training Loss: 63.9205\n",
      "Training Loss: 65.4122\n",
      "Training Loss: 62.3662\n",
      "Training Loss: 68.2885\n",
      "Training Loss: 70.3538\n",
      "Training Loss: 67.9649\n",
      "Training Loss: 62.0683\n",
      "Training Loss: 63.5325\n",
      "Training Loss: 63.1475\n",
      "Training Loss: 62.0158\n",
      "Training Loss: 64.5601\n",
      "Training Loss: 62.9599\n",
      "Training Loss: 66.4336\n",
      "Training Loss: 67.2484\n",
      "Training Loss: 63.5939\n",
      "Training Loss: 67.6189\n",
      "Training Loss: 70.1692\n",
      "Training Loss: 68.2174\n",
      "Training Loss: 62.2137\n",
      "Training Loss: 71.2201\n",
      "Training Loss: 75.6603\n",
      "Training Loss: 75.2688\n",
      "Training Loss: 70.5291\n",
      "Training Loss: 61.8761\n",
      "Training Loss: 73.7342\n",
      "Training Loss: 80.2678\n",
      "Training Loss: 81.9015\n",
      "Training Loss: 79.1252\n",
      "Training Loss: 72.3799\n",
      "Training Loss: 62.0625\n",
      "Training Loss: 75.3193\n",
      "Training Loss: 83.3109\n",
      "Training Loss: 86.1161\n",
      "Training Loss: 84.2539\n",
      "Training Loss: 78.1911\n",
      "Training Loss: 68.3480\n",
      "Training Loss: 68.4201\n",
      "Training Loss: 75.9022\n",
      "Training Loss: 78.3900\n",
      "Training Loss: 76.3831\n",
      "Training Loss: 70.3309\n",
      "Training Loss: 63.0129\n",
      "Training Loss: 67.4932\n",
      "Training Loss: 67.1389\n",
      "Training Loss: 62.4338\n",
      "Training Loss: 69.6890\n",
      "Training Loss: 73.0767\n",
      "Training Loss: 71.8800\n",
      "Training Loss: 66.5573\n",
      "Training Loss: 66.1806\n",
      "Training Loss: 70.0603\n",
      "Training Loss: 69.1659\n",
      "Training Loss: 63.9750\n",
      "Training Loss: 68.6024\n",
      "Training Loss: 72.3772\n",
      "Training Loss: 71.5293\n",
      "Training Loss: 66.5209\n",
      "Training Loss: 65.9298\n",
      "Training Loss: 69.5508\n",
      "Training Loss: 68.4239\n",
      "Training Loss: 63.0243\n",
      "Training Loss: 69.7223\n",
      "Training Loss: 73.6635\n",
      "Training Loss: 72.9657\n",
      "Training Loss: 68.0928\n",
      "Training Loss: 64.2077\n",
      "Training Loss: 67.7172\n",
      "Training Loss: 66.4903\n",
      "Training Loss: 62.6171\n",
      "Training Loss: 63.2331\n",
      "Training Loss: 64.1259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 63.1166\n",
      "Training Loss: 65.7437\n",
      "Training Loss: 66.1863\n",
      "Training Loss: 62.3402\n",
      "Training Loss: 69.1150\n",
      "Training Loss: 71.7791\n",
      "Training Loss: 69.7920\n",
      "Training Loss: 63.6189\n",
      "Training Loss: 69.8212\n",
      "Training Loss: 74.3788\n",
      "Training Loss: 74.2366\n",
      "Training Loss: 69.8645\n",
      "Training Loss: 61.9480\n",
      "Training Loss: 65.0452\n",
      "Training Loss: 63.4482\n",
      "Training Loss: 65.9370\n",
      "Training Loss: 66.8480\n",
      "Training Loss: 63.4240\n",
      "Training Loss: 67.6266\n",
      "Training Loss: 69.9433\n",
      "Training Loss: 67.6441\n",
      "Training Loss: 62.4304\n",
      "Training Loss: 63.9010\n",
      "Training Loss: 62.6641\n",
      "Training Loss: 62.6661\n",
      "Training Loss: 63.7229\n",
      "Training Loss: 61.8334\n",
      "Training Loss: 67.7843\n",
      "Training Loss: 68.9285\n",
      "Training Loss: 65.7149\n",
      "Training Loss: 65.1052\n",
      "Training Loss: 67.2487\n",
      "Training Loss: 64.7943\n",
      "Training Loss: 65.3712\n",
      "Training Loss: 66.9656\n",
      "Training Loss: 64.1575\n",
      "Training Loss: 66.3170\n",
      "Training Loss: 68.1268\n",
      "Training Loss: 65.3722\n",
      "Training Loss: 65.0681\n",
      "Training Loss: 66.9017\n",
      "Training Loss: 64.3091\n",
      "Training Loss: 65.9655\n",
      "Training Loss: 67.5979\n",
      "Training Loss: 64.6839\n",
      "Training Loss: 65.8862\n",
      "Training Loss: 67.8468\n",
      "Training Loss: 65.3688\n",
      "Training Loss: 64.7832\n",
      "Training Loss: 66.3213\n",
      "Training Loss: 63.3227\n",
      "Training Loss: 67.3001\n",
      "Training Loss: 69.3282\n",
      "Training Loss: 66.9112\n",
      "Training Loss: 63.1596\n",
      "Training Loss: 64.6475\n",
      "Training Loss: 62.0240\n",
      "Training Loss: 63.1766\n",
      "Training Loss: 63.3960\n",
      "Training Loss: 61.8507\n",
      "Training Loss: 64.6397\n",
      "Training Loss: 62.9703\n",
      "Training Loss: 66.4809\n",
      "Training Loss: 67.3534\n",
      "Training Loss: 63.7563\n",
      "Training Loss: 67.4029\n",
      "Training Loss: 69.9079\n",
      "Training Loss: 67.9207\n",
      "Training Loss: 61.8905\n",
      "Training Loss: 71.5675\n",
      "Training Loss: 76.0302\n",
      "Training Loss: 75.6645\n",
      "Training Loss: 70.9534\n",
      "Training Loss: 62.3316\n",
      "Training Loss: 73.2531\n",
      "Training Loss: 79.7624\n",
      "Training Loss: 81.3793\n",
      "Training Loss: 78.5933\n",
      "Training Loss: 71.8444\n",
      "Training Loss: 62.1070\n",
      "Training Loss: 67.1614\n",
      "Training Loss: 67.3285\n",
      "Training Loss: 63.0974\n",
      "Training Loss: 68.6117\n",
      "Training Loss: 71.6221\n",
      "Training Loss: 70.0903\n",
      "Training Loss: 64.4707\n",
      "Training Loss: 68.5685\n",
      "Training Loss: 72.6932\n",
      "Training Loss: 72.0241\n",
      "Training Loss: 67.0406\n",
      "Training Loss: 65.3982\n",
      "Training Loss: 69.0081\n",
      "Training Loss: 68.0163\n",
      "Training Loss: 62.8829\n",
      "Training Loss: 69.7373\n",
      "Training Loss: 73.4617\n",
      "Training Loss: 72.4326\n",
      "Training Loss: 67.1254\n",
      "Training Loss: 65.6009\n",
      "Training Loss: 69.4688\n",
      "Training Loss: 68.7094\n",
      "Training Loss: 63.7855\n",
      "Training Loss: 68.6279\n",
      "Training Loss: 72.1799\n",
      "Training Loss: 70.9959\n",
      "Training Loss: 65.5497\n",
      "Training Loss: 67.2741\n",
      "Training Loss: 71.2528\n",
      "Training Loss: 70.5935\n",
      "Training Loss: 65.7600\n",
      "Training Loss: 66.5381\n",
      "Training Loss: 70.0157\n",
      "Training Loss: 68.7651\n",
      "Training Loss: 63.2592\n",
      "Training Loss: 69.5800\n",
      "Training Loss: 73.6063\n",
      "Training Loss: 72.9901\n",
      "Training Loss: 68.1957\n",
      "Training Loss: 64.0263\n",
      "Training Loss: 67.4718\n",
      "Training Loss: 66.1926\n",
      "Training Loss: 62.9514\n",
      "Training Loss: 63.6096\n",
      "Training Loss: 63.6996\n",
      "Training Loss: 62.6561\n",
      "Training Loss: 66.2219\n",
      "Training Loss: 66.6922\n",
      "Training Loss: 62.8760\n",
      "Training Loss: 68.5382\n",
      "Training Loss: 71.1785\n",
      "Training Loss: 69.1748\n",
      "Training Loss: 62.9919\n",
      "Training Loss: 70.4418\n",
      "Training Loss: 75.0077\n",
      "Training Loss: 74.8778\n",
      "Training Loss: 70.5216\n",
      "Training Loss: 62.3617\n",
      "Training Loss: 73.0341\n",
      "Training Loss: 79.2504\n",
      "Training Loss: 80.4654\n",
      "Training Loss: 77.1795\n",
      "Training Loss: 69.8429\n",
      "Training Loss: 64.7227\n",
      "Training Loss: 70.2080\n",
      "Training Loss: 70.9059\n",
      "Training Loss: 67.2950\n",
      "Training Loss: 63.8580\n",
      "Training Loss: 66.3292\n",
      "Training Loss: 64.1741\n",
      "Training Loss: 65.7120\n",
      "Training Loss: 67.0682\n",
      "Training Loss: 64.0500\n",
      "Training Loss: 66.6139\n",
      "Training Loss: 68.5971\n",
      "Training Loss: 66.0030\n",
      "Training Loss: 64.3010\n",
      "Training Loss: 66.0070\n",
      "Training Loss: 63.3039\n",
      "Training Loss: 67.0839\n",
      "Training Loss: 68.8078\n",
      "Training Loss: 65.9805\n",
      "Training Loss: 64.5294\n",
      "Training Loss: 66.4212\n",
      "Training Loss: 63.8856\n",
      "Training Loss: 66.3393\n",
      "Training Loss: 67.9253\n",
      "Training Loss: 64.9742\n",
      "Training Loss: 65.6290\n",
      "Training Loss: 67.6195\n",
      "Training Loss: 65.1729\n",
      "Training Loss: 64.9491\n",
      "Training Loss: 66.4619\n",
      "Training Loss: 63.4450\n",
      "Training Loss: 67.1914\n",
      "Training Loss: 69.2344\n",
      "Training Loss: 66.8352\n",
      "Training Loss: 63.2161\n",
      "Training Loss: 64.6898\n",
      "Training Loss: 61.9904\n",
      "Training Loss: 63.1989\n",
      "Training Loss: 63.3801\n",
      "Training Loss: 61.8569\n",
      "Training Loss: 64.6380\n",
      "Training Loss: 62.9769\n",
      "Training Loss: 66.4623\n",
      "Training Loss: 67.3284\n",
      "Training Loss: 63.7301\n",
      "Training Loss: 67.4254\n",
      "Training Loss: 69.9318\n",
      "Training Loss: 67.9501\n",
      "Training Loss: 61.9291\n",
      "Training Loss: 71.5155\n",
      "Training Loss: 75.9711\n",
      "Training Loss: 75.6034\n",
      "Training Loss: 70.8949\n",
      "Training Loss: 62.2798\n",
      "Training Loss: 73.2940\n",
      "Training Loss: 79.7983\n",
      "Training Loss: 81.4150\n",
      "Training Loss: 78.6329\n",
      "Training Loss: 71.8918\n",
      "Training Loss: 62.0476\n",
      "Training Loss: 67.0961\n",
      "Training Loss: 67.2623\n",
      "Training Loss: 63.0345\n",
      "Training Loss: 68.6664\n",
      "Training Loss: 71.6744\n",
      "Training Loss: 70.1447\n",
      "Training Loss: 64.5310\n",
      "Training Loss: 68.4977\n",
      "Training Loss: 72.6180\n",
      "Training Loss: 71.9491\n",
      "Training Loss: 66.9701\n",
      "Training Loss: 65.4594\n",
      "Training Loss: 69.0662\n",
      "Training Loss: 68.0755\n",
      "Training Loss: 62.9473\n",
      "Training Loss: 69.6631\n",
      "Training Loss: 73.3838\n",
      "Training Loss: 72.3555\n",
      "Training Loss: 67.0533\n",
      "Training Loss: 65.6635\n",
      "Training Loss: 69.5278\n",
      "Training Loss: 68.7693\n",
      "Training Loss: 63.8501\n",
      "Training Loss: 68.5539\n",
      "Training Loss: 72.1025\n",
      "Training Loss: 70.9195\n",
      "Training Loss: 65.4783\n",
      "Training Loss: 67.3358\n",
      "Training Loss: 71.3109\n",
      "Training Loss: 70.6523\n",
      "Training Loss: 65.8233\n",
      "Training Loss: 66.4656\n",
      "Training Loss: 69.9400\n",
      "Training Loss: 68.6906\n",
      "Training Loss: 63.1897\n",
      "Training Loss: 69.6400\n",
      "Training Loss: 73.6627\n",
      "Training Loss: 73.0470\n",
      "Training Loss: 68.2570\n",
      "Training Loss: 63.9561\n",
      "Training Loss: 67.3984\n",
      "Training Loss: 66.1204\n",
      "Training Loss: 63.0176\n",
      "Training Loss: 63.6751\n",
      "Training Loss: 63.6296\n",
      "Training Loss: 62.5870\n",
      "Training Loss: 66.2852\n",
      "Training Loss: 66.7551\n",
      "Training Loss: 62.9422\n",
      "Training Loss: 68.4639\n",
      "Training Loss: 71.1017\n",
      "Training Loss: 69.0999\n",
      "Training Loss: 62.9225\n",
      "Training Loss: 70.5015\n",
      "Training Loss: 75.0633\n",
      "Training Loss: 74.9335\n",
      "Training Loss: 70.5812\n",
      "Training Loss: 62.4285\n",
      "Training Loss: 72.9558\n",
      "Training Loss: 79.1666\n",
      "Training Loss: 80.3806\n",
      "Training Loss: 77.0976\n",
      "Training Loss: 69.7674\n",
      "Training Loss: 64.7876\n",
      "Training Loss: 70.2681\n",
      "Training Loss: 70.9654\n",
      "Training Loss: 67.3577\n",
      "Training Loss: 63.7878\n",
      "Training Loss: 66.2568\n",
      "Training Loss: 64.1035\n",
      "Training Loss: 65.7760\n",
      "Training Loss: 67.1311\n",
      "Training Loss: 64.1155\n",
      "Training Loss: 66.5413\n",
      "Training Loss: 68.5228\n",
      "Training Loss: 65.9309\n",
      "Training Loss: 64.3663\n",
      "Training Loss: 66.0709\n",
      "Training Loss: 63.3701\n",
      "Training Loss: 67.0109\n",
      "Training Loss: 68.7333\n",
      "Training Loss: 65.9085\n",
      "Training Loss: 64.5946\n",
      "Training Loss: 66.4848\n",
      "Training Loss: 63.9513\n",
      "Training Loss: 66.2669\n",
      "Training Loss: 67.8516\n",
      "Training Loss: 64.9029\n",
      "Training Loss: 65.6933\n",
      "Training Loss: 67.6822\n",
      "Training Loss: 65.2376\n",
      "Training Loss: 64.8780\n",
      "Training Loss: 66.3895\n",
      "Training Loss: 63.3751\n",
      "Training Loss: 67.2545\n",
      "Training Loss: 69.2958\n",
      "Training Loss: 66.8985\n",
      "Training Loss: 63.1463\n",
      "Training Loss: 64.6189\n",
      "Training Loss: 62.0578\n",
      "Training Loss: 63.1292\n",
      "Training Loss: 63.4463\n",
      "Training Loss: 61.8427\n",
      "Training Loss: 67.5591\n",
      "Training Loss: 68.3783\n",
      "Training Loss: 64.7412\n",
      "Training Loss: 66.4615\n",
      "Training Loss: 68.9991\n",
      "Training Loss: 67.0488\n",
      "Training Loss: 62.5844\n",
      "Training Loss: 63.6890\n",
      "Training Loss: 63.2981\n",
      "Training Loss: 62.0570\n",
      "Training Loss: 67.0093\n",
      "Training Loss: 67.5301\n",
      "Training Loss: 63.6247\n",
      "Training Loss: 67.7974\n",
      "Training Loss: 70.5488\n",
      "Training Loss: 68.7912\n",
      "Training Loss: 62.9755\n",
      "Training Loss: 70.2604\n",
      "Training Loss: 74.5474\n",
      "Training Loss: 74.0317\n",
      "Training Loss: 69.1936\n",
      "Training Loss: 63.1439\n",
      "Training Loss: 66.6386\n",
      "Training Loss: 65.5501\n",
      "Training Loss: 63.3188\n",
      "Training Loss: 63.7140\n",
      "Training Loss: 63.9013\n",
      "Training Loss: 63.2256\n",
      "Training Loss: 65.3038\n",
      "Training Loss: 65.3591\n",
      "Training Loss: 62.5835\n",
      "Training Loss: 62.1787\n",
      "Training Loss: 66.1201\n",
      "Training Loss: 65.9525\n",
      "Training Loss: 62.1971\n",
      "Training Loss: 61.9699\n",
      "Training Loss: 66.1696\n",
      "Training Loss: 65.8557\n",
      "Training Loss: 62.4217\n",
      "Training Loss: 62.3111\n",
      "Training Loss: 65.7161\n",
      "Training Loss: 65.3062\n",
      "Training Loss: 63.0472\n",
      "Training Loss: 63.0131\n",
      "Training Loss: 64.9326\n",
      "Training Loss: 64.4597\n",
      "Training Loss: 63.9357\n",
      "Training Loss: 63.9517\n",
      "Training Loss: 63.9325\n",
      "Training Loss: 63.4184\n",
      "Training Loss: 64.9967\n",
      "Training Loss: 65.0457\n",
      "Training Loss: 62.7904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 62.2492\n",
      "Training Loss: 66.1709\n",
      "Training Loss: 66.2415\n",
      "Training Loss: 62.0721\n",
      "Training Loss: 69.6717\n",
      "Training Loss: 72.6035\n",
      "Training Loss: 70.8690\n",
      "Training Loss: 64.9351\n",
      "Training Loss: 68.3031\n",
      "Training Loss: 72.6711\n",
      "Training Loss: 72.3697\n",
      "Training Loss: 67.8656\n",
      "Training Loss: 64.0890\n",
      "Training Loss: 67.2963\n",
      "Training Loss: 65.8100\n",
      "Training Loss: 63.5040\n",
      "Training Loss: 64.3277\n",
      "Training Loss: 62.8110\n",
      "Training Loss: 61.9964\n",
      "Training Loss: 64.8729\n",
      "Training Loss: 63.4171\n",
      "Training Loss: 65.8312\n",
      "Training Loss: 66.6307\n",
      "Training Loss: 63.1178\n",
      "Training Loss: 68.0078\n",
      "Training Loss: 70.3992\n",
      "Training Loss: 68.1788\n",
      "Training Loss: 61.8232\n",
      "Training Loss: 63.2319\n",
      "Training Loss: 63.3894\n",
      "Training Loss: 61.9012\n",
      "Training Loss: 64.5360\n",
      "Training Loss: 62.6901\n",
      "Training Loss: 66.8916\n",
      "Training Loss: 68.0019\n",
      "Training Loss: 64.7690\n",
      "Training Loss: 66.0730\n",
      "Training Loss: 68.2338\n",
      "Training Loss: 65.8062\n",
      "Training Loss: 64.3406\n",
      "Training Loss: 65.9145\n",
      "Training Loss: 63.0989\n",
      "Training Loss: 67.3887\n",
      "Training Loss: 69.2059\n",
      "Training Loss: 66.4693\n",
      "Training Loss: 63.9618\n",
      "Training Loss: 65.7819\n",
      "Training Loss: 63.1881\n",
      "Training Loss: 67.0949\n",
      "Training Loss: 68.7296\n",
      "Training Loss: 65.8288\n",
      "Training Loss: 64.7372\n",
      "Training Loss: 66.6882\n",
      "Training Loss: 64.2123\n",
      "Training Loss: 65.9459\n",
      "Training Loss: 67.4836\n",
      "Training Loss: 64.4955\n",
      "Training Loss: 66.1260\n",
      "Training Loss: 68.1466\n",
      "Training Loss: 65.7334\n",
      "Training Loss: 64.3425\n",
      "Training Loss: 65.8285\n",
      "Training Loss: 62.7942\n",
      "Training Loss: 67.8408\n",
      "Training Loss: 69.8982\n",
      "Training Loss: 67.5184\n",
      "Training Loss: 62.4975\n",
      "Training Loss: 63.9561\n",
      "Training Loss: 62.7193\n",
      "Training Loss: 62.4452\n",
      "Training Loss: 64.1267\n",
      "Training Loss: 62.5325\n",
      "Training Loss: 66.8464\n",
      "Training Loss: 67.6582\n",
      "Training Loss: 64.0173\n",
      "Training Loss: 67.1742\n",
      "Training Loss: 69.7151\n",
      "Training Loss: 67.7705\n",
      "Training Loss: 61.8427\n",
      "Training Loss: 62.9430\n",
      "Training Loss: 64.0331\n",
      "Training Loss: 62.7958\n",
      "Training Loss: 66.2522\n",
      "Training Loss: 66.7702\n",
      "Training Loss: 62.8651\n",
      "Training Loss: 68.5415\n",
      "Training Loss: 71.2928\n",
      "Training Loss: 69.5380\n",
      "Training Loss: 63.7274\n",
      "Training Loss: 69.4886\n",
      "Training Loss: 73.7717\n",
      "Training Loss: 73.2552\n",
      "Training Loss: 68.4194\n",
      "Training Loss: 63.9010\n",
      "Training Loss: 67.3941\n",
      "Training Loss: 66.3070\n",
      "Training Loss: 62.5455\n",
      "Training Loss: 62.9398\n",
      "Training Loss: 64.6609\n",
      "Training Loss: 63.9861\n",
      "Training Loss: 64.5273\n",
      "Training Loss: 64.5822\n",
      "Training Loss: 63.3455\n",
      "Training Loss: 62.9412\n",
      "Training Loss: 65.3419\n",
      "Training Loss: 65.1741\n",
      "Training Loss: 62.9603\n",
      "Training Loss: 62.7335\n",
      "Training Loss: 65.3905\n",
      "Training Loss: 65.0767\n",
      "Training Loss: 63.1854\n",
      "Training Loss: 63.0750\n",
      "Training Loss: 64.9368\n",
      "Training Loss: 64.5270\n",
      "Training Loss: 63.8110\n",
      "Training Loss: 63.7770\n",
      "Training Loss: 64.1534\n",
      "Training Loss: 63.6807\n",
      "Training Loss: 64.6992\n",
      "Training Loss: 64.7153\n",
      "Training Loss: 63.1537\n",
      "Training Loss: 62.6398\n",
      "Training Loss: 65.7597\n",
      "Training Loss: 65.8087\n",
      "Training Loss: 62.0122\n",
      "Training Loss: 62.1546\n",
      "Training Loss: 65.2842\n",
      "Training Loss: 64.3455\n",
      "Training Loss: 64.4577\n",
      "Training Loss: 64.8453\n",
      "Training Loss: 62.6815\n",
      "Training Loss: 61.8618\n",
      "Training Loss: 66.7957\n",
      "Training Loss: 67.0884\n",
      "Training Loss: 63.1217\n",
      "Training Loss: 68.4168\n",
      "Training Loss: 71.1820\n",
      "Training Loss: 69.3005\n",
      "Training Loss: 63.2370\n",
      "Training Loss: 70.0857\n",
      "Training Loss: 74.5573\n",
      "Training Loss: 74.3516\n",
      "Training Loss: 69.9365\n",
      "Training Loss: 61.9001\n",
      "Training Loss: 65.0345\n",
      "Training Loss: 63.4854\n",
      "Training Loss: 65.8441\n",
      "Training Loss: 66.7181\n",
      "Training Loss: 63.2747\n",
      "Training Loss: 67.7821\n",
      "Training Loss: 70.1164\n",
      "Training Loss: 67.8473\n",
      "Training Loss: 62.1899\n",
      "Training Loss: 63.6377\n",
      "Training Loss: 62.9387\n",
      "Training Loss: 62.3760\n",
      "Training Loss: 64.0222\n",
      "Training Loss: 62.1508\n",
      "Training Loss: 67.4422\n",
      "Training Loss: 68.5731\n",
      "Training Loss: 65.3611\n",
      "Training Loss: 65.4495\n",
      "Training Loss: 67.5934\n",
      "Training Loss: 65.1531\n",
      "Training Loss: 64.9920\n",
      "Training Loss: 66.5762\n",
      "Training Loss: 63.7724\n",
      "Training Loss: 66.6909\n",
      "Training Loss: 68.4988\n",
      "Training Loss: 65.7562\n",
      "Training Loss: 64.6662\n",
      "Training Loss: 66.4913\n",
      "Training Loss: 63.9044\n",
      "Training Loss: 66.3582\n",
      "Training Loss: 67.9875\n",
      "Training Loss: 65.0844\n",
      "Training Loss: 65.4692\n",
      "Training Loss: 67.4223\n",
      "Training Loss: 64.9507\n",
      "Training Loss: 65.1891\n",
      "Training Loss: 66.7235\n",
      "Training Loss: 63.7351\n",
      "Training Loss: 66.8722\n",
      "Training Loss: 68.8933\n",
      "Training Loss: 66.4830\n",
      "Training Loss: 63.5755\n",
      "Training Loss: 65.0595\n",
      "Training Loss: 62.0257\n",
      "Training Loss: 68.5939\n",
      "Training Loss: 70.6511\n",
      "Training Loss: 68.2734\n",
      "Training Loss: 61.9042\n",
      "Training Loss: 71.8509\n",
      "Training Loss: 76.5942\n",
      "Training Loss: 76.4939\n",
      "Training Loss: 72.0345\n",
      "Training Loss: 63.6520\n",
      "Training Loss: 71.7298\n",
      "Training Loss: 78.0494\n",
      "Training Loss: 79.5081\n",
      "Training Loss: 76.5919\n",
      "Training Loss: 69.7383\n",
      "Training Loss: 64.3312\n",
      "Training Loss: 69.4734\n",
      "Training Loss: 69.7322\n",
      "Training Loss: 65.5962\n",
      "Training Loss: 66.0568\n",
      "Training Loss: 68.9924\n",
      "Training Loss: 67.4055\n",
      "Training Loss: 61.8840\n",
      "Training Loss: 62.6901\n",
      "Training Loss: 64.5400\n",
      "Training Loss: 63.5373\n",
      "Training Loss: 65.2813\n",
      "Training Loss: 65.6064\n",
      "Training Loss: 62.0965\n",
      "Training Loss: 62.1599\n",
      "Training Loss: 65.2177\n",
      "Training Loss: 64.3556\n",
      "Training Loss: 64.3209\n",
      "Training Loss: 64.5303\n",
      "Training Loss: 63.2576\n",
      "Training Loss: 62.7304\n",
      "Training Loss: 65.6664\n",
      "Training Loss: 65.6000\n",
      "Training Loss: 62.4493\n",
      "Training Loss: 62.1418\n",
      "Training Loss: 66.0636\n",
      "Training Loss: 65.8164\n",
      "Training Loss: 62.3965\n",
      "Training Loss: 62.2331\n",
      "Training Loss: 65.8388\n",
      "Training Loss: 65.4729\n",
      "Training Loss: 62.8394\n",
      "Training Loss: 62.7707\n",
      "Training Loss: 65.2059\n",
      "Training Loss: 64.7621\n",
      "Training Loss: 63.6076\n",
      "Training Loss: 63.6009\n",
      "Training Loss: 64.3051\n",
      "Training Loss: 63.8103\n",
      "Training Loss: 64.5892\n",
      "Training Loss: 64.6232\n",
      "Training Loss: 63.2287\n",
      "Training Loss: 62.7004\n",
      "Training Loss: 65.7108\n",
      "Training Loss: 65.7715\n",
      "Training Loss: 62.0371\n",
      "Training Loss: 62.1393\n",
      "Training Loss: 65.2892\n",
      "Training Loss: 64.3430\n",
      "Training Loss: 64.4645\n",
      "Training Loss: 64.8581\n",
      "Training Loss: 62.6608\n",
      "Training Loss: 61.8364\n",
      "Training Loss: 66.8228\n",
      "Training Loss: 67.1195\n",
      "Training Loss: 63.1583\n",
      "Training Loss: 68.3722\n",
      "Training Loss: 71.1331\n",
      "Training Loss: 69.2498\n",
      "Training Loss: 63.1868\n",
      "Training Loss: 70.1326\n",
      "Training Loss: 74.6040\n",
      "Training Loss: 74.4002\n",
      "Training Loss: 69.9887\n",
      "Training Loss: 61.8414\n",
      "Training Loss: 64.9730\n",
      "Training Loss: 63.4235\n",
      "Training Loss: 65.9033\n",
      "Training Loss: 66.7778\n",
      "Training Loss: 63.3370\n",
      "Training Loss: 67.7145\n",
      "Training Loss: 70.0470\n",
      "Training Loss: 67.7784\n",
      "Training Loss: 62.2553\n",
      "Training Loss: 63.7029\n",
      "Training Loss: 62.8705\n",
      "Training Loss: 62.4426\n",
      "Training Loss: 63.9528\n",
      "Training Loss: 62.0820\n",
      "Training Loss: 67.5074\n",
      "Training Loss: 68.6380\n",
      "Training Loss: 65.4278\n",
      "Training Loss: 65.3781\n",
      "Training Loss: 67.5207\n",
      "Training Loss: 65.0814\n",
      "Training Loss: 65.0596\n",
      "Training Loss: 66.6433\n",
      "Training Loss: 63.8409\n",
      "Training Loss: 66.6180\n",
      "Training Loss: 68.4249\n",
      "Training Loss: 65.6835\n",
      "Training Loss: 64.7346\n",
      "Training Loss: 66.5590\n",
      "Training Loss: 63.9733\n",
      "Training Loss: 66.2850\n",
      "Training Loss: 67.9135\n",
      "Training Loss: 65.0116\n",
      "Training Loss: 65.5377\n",
      "Training Loss: 67.4899\n",
      "Training Loss: 65.0195\n",
      "Training Loss: 65.1162\n",
      "Training Loss: 66.6498\n",
      "Training Loss: 63.6627\n",
      "Training Loss: 66.9402\n",
      "Training Loss: 68.9604\n",
      "Training Loss: 66.5512\n",
      "Training Loss: 63.5032\n",
      "Training Loss: 64.9864\n",
      "Training Loss: 61.9540\n",
      "Training Loss: 68.6613\n",
      "Training Loss: 70.7176\n",
      "Training Loss: 68.3410\n",
      "Training Loss: 61.9746\n",
      "Training Loss: 71.7748\n",
      "Training Loss: 76.5159\n",
      "Training Loss: 76.4156\n",
      "Training Loss: 71.9582\n",
      "Training Loss: 63.5794\n",
      "Training Loss: 71.7959\n",
      "Training Loss: 78.1127\n",
      "Training Loss: 79.5708\n",
      "Training Loss: 76.6559\n",
      "Training Loss: 69.8053\n",
      "Training Loss: 64.2584\n",
      "Training Loss: 69.3983\n",
      "Training Loss: 69.6570\n",
      "Training Loss: 65.5228\n",
      "Training Loss: 66.1255\n",
      "Training Loss: 69.0598\n",
      "Training Loss: 67.4737\n",
      "Training Loss: 61.8190\n",
      "Training Loss: 71.2816\n",
      "Training Loss: 75.4368\n",
      "Training Loss: 74.8095\n",
      "Training Loss: 69.8780\n",
      "Training Loss: 62.5466\n",
      "Training Loss: 66.1163\n",
      "Training Loss: 65.1021\n",
      "Training Loss: 63.6994\n",
      "Training Loss: 64.0341\n",
      "Training Loss: 63.6329\n",
      "Training Loss: 63.0059\n",
      "Training Loss: 65.4758\n",
      "Training Loss: 65.4918\n",
      "Training Loss: 62.4810\n",
      "Training Loss: 62.1081\n",
      "Training Loss: 66.1560\n",
      "Training Loss: 65.9629\n",
      "Training Loss: 62.2027\n",
      "Training Loss: 61.9964\n",
      "Training Loss: 66.1170\n",
      "Training Loss: 65.7867\n",
      "Training Loss: 62.4974\n",
      "Training Loss: 62.4005\n",
      "Training Loss: 65.6062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 65.1858\n",
      "Training Loss: 63.1682\n",
      "Training Loss: 63.1430\n",
      "Training Loss: 64.7858\n",
      "Training Loss: 64.3064\n",
      "Training Loss: 64.0857\n",
      "Training Loss: 64.1075\n",
      "Training Loss: 63.7623\n",
      "Training Loss: 63.2442\n",
      "Training Loss: 65.1650\n",
      "Training Loss: 65.2177\n",
      "Training Loss: 62.6055\n",
      "Training Loss: 62.0620\n",
      "Training Loss: 66.3505\n",
      "Training Loss: 66.4235\n",
      "Training Loss: 62.2626\n",
      "Training Loss: 69.4637\n",
      "Training Loss: 72.3891\n",
      "Training Loss: 70.6556\n",
      "Training Loss: 64.7291\n",
      "Training Loss: 68.4927\n",
      "Training Loss: 72.8554\n",
      "Training Loss: 72.5554\n",
      "Training Loss: 68.0591\n",
      "Training Loss: 63.8789\n",
      "Training Loss: 67.0806\n",
      "Training Loss: 65.5958\n",
      "Training Loss: 63.7070\n",
      "Training Loss: 64.5301\n",
      "Training Loss: 62.5995\n",
      "Training Loss: 62.2033\n",
      "Training Loss: 64.6575\n",
      "Training Loss: 63.2034\n",
      "Training Loss: 66.0335\n",
      "Training Loss: 66.8321\n",
      "Training Loss: 63.3246\n",
      "Training Loss: 67.7863\n",
      "Training Loss: 70.1739\n",
      "Training Loss: 67.9566\n",
      "Training Loss: 62.0329\n",
      "Training Loss: 63.4397\n",
      "Training Loss: 63.1736\n",
      "Training Loss: 62.1112\n",
      "Training Loss: 64.3183\n",
      "Training Loss: 62.4750\n",
      "Training Loss: 67.0947\n",
      "Training Loss: 68.2035\n",
      "Training Loss: 64.9754\n",
      "Training Loss: 65.8527\n",
      "Training Loss: 68.0104\n",
      "Training Loss: 65.5862\n",
      "Training Loss: 64.5479\n",
      "Training Loss: 66.1196\n",
      "Training Loss: 63.3080\n",
      "Training Loss: 67.1662\n",
      "Training Loss: 68.9809\n",
      "Training Loss: 66.2481\n",
      "Training Loss: 64.1699\n",
      "Training Loss: 65.9875\n",
      "Training Loss: 63.3974\n",
      "Training Loss: 66.8728\n",
      "Training Loss: 68.5051\n",
      "Training Loss: 65.6084\n",
      "Training Loss: 64.9444\n",
      "Training Loss: 66.8927\n",
      "Training Loss: 64.4203\n",
      "Training Loss: 65.7253\n",
      "Training Loss: 67.2608\n",
      "Training Loss: 64.2769\n",
      "Training Loss: 66.3314\n",
      "Training Loss: 68.3491\n",
      "Training Loss: 65.9393\n",
      "Training Loss: 64.1241\n",
      "Training Loss: 65.6080\n",
      "Training Loss: 62.5779\n",
      "Training Loss: 68.0439\n",
      "Training Loss: 70.0985\n",
      "Training Loss: 67.7219\n",
      "Training Loss: 62.2817\n",
      "Training Loss: 63.7382\n",
      "Training Loss: 62.9295\n",
      "Training Loss: 62.2294\n",
      "Training Loss: 64.3349\n",
      "Training Loss: 62.7429\n",
      "Training Loss: 66.6246\n",
      "Training Loss: 67.4353\n",
      "Training Loss: 63.7994\n",
      "Training Loss: 67.3784\n",
      "Training Loss: 69.9158\n",
      "Training Loss: 67.9739\n",
      "Training Loss: 62.0006\n",
      "Training Loss: 71.3871\n",
      "Training Loss: 75.8050\n",
      "Training Loss: 75.4155\n",
      "Training Loss: 70.6995\n",
      "Training Loss: 62.0898\n",
      "Training Loss: 73.4642\n",
      "Training Loss: 79.9652\n",
      "Training Loss: 81.5907\n",
      "Training Loss: 78.8282\n",
      "Training Loss: 72.1165\n",
      "Training Loss: 61.8503\n",
      "Training Loss: 75.4668\n",
      "Training Loss: 83.4188\n",
      "Training Loss: 86.2102\n",
      "Training Loss: 84.3571\n",
      "Training Loss: 78.3241\n",
      "Training Loss: 68.5292\n",
      "Training Loss: 68.1781\n",
      "Training Loss: 75.6236\n",
      "Training Loss: 78.0994\n",
      "Training Loss: 76.1022\n",
      "Training Loss: 70.0794\n",
      "Training Loss: 63.2206\n",
      "Training Loss: 67.6792\n",
      "Training Loss: 67.3266\n",
      "Training Loss: 62.6442\n",
      "Training Loss: 69.4412\n",
      "Training Loss: 72.8126\n",
      "Training Loss: 71.6217\n",
      "Training Loss: 66.3245\n",
      "Training Loss: 66.3732\n",
      "Training Loss: 70.2343\n",
      "Training Loss: 69.3442\n",
      "Training Loss: 64.1780\n",
      "Training Loss: 68.3604\n",
      "Training Loss: 72.1173\n",
      "Training Loss: 71.2733\n",
      "Training Loss: 66.2887\n",
      "Training Loss: 66.1238\n",
      "Training Loss: 69.7277\n",
      "Training Loss: 68.6062\n",
      "Training Loss: 63.2318\n",
      "Training Loss: 69.4755\n",
      "Training Loss: 73.3983\n",
      "Training Loss: 72.7038\n",
      "Training Loss: 67.8536\n",
      "Training Loss: 64.4099\n",
      "Training Loss: 67.9031\n",
      "Training Loss: 66.6820\n",
      "Training Loss: 62.4036\n",
      "Training Loss: 63.0167\n",
      "Training Loss: 64.3285\n",
      "Training Loss: 63.3238\n",
      "Training Loss: 65.5158\n",
      "Training Loss: 65.9564\n",
      "Training Loss: 62.1280\n",
      "Training Loss: 69.2948\n",
      "Training Loss: 71.9468\n",
      "Training Loss: 69.9687\n",
      "Training Loss: 63.8237\n",
      "Training Loss: 69.5753\n",
      "Training Loss: 74.1123\n",
      "Training Loss: 73.9707\n",
      "Training Loss: 69.6184\n",
      "Training Loss: 62.1605\n",
      "Training Loss: 65.2438\n",
      "Training Loss: 63.6540\n",
      "Training Loss: 65.7088\n",
      "Training Loss: 66.6157\n",
      "Training Loss: 63.2071\n",
      "Training Loss: 67.8138\n",
      "Training Loss: 70.1201\n",
      "Training Loss: 67.8312\n",
      "Training Loss: 62.2182\n",
      "Training Loss: 63.6822\n",
      "Training Loss: 62.8734\n",
      "Training Loss: 62.4528\n",
      "Training Loss: 63.9274\n",
      "Training Loss: 62.0462\n",
      "Training Loss: 67.5486\n",
      "Training Loss: 68.6877\n",
      "Training Loss: 65.4882\n",
      "Training Loss: 65.3038\n",
      "Training Loss: 67.4380\n",
      "Training Loss: 64.9942\n",
      "Training Loss: 65.1463\n",
      "Training Loss: 66.7337\n",
      "Training Loss: 63.9378\n",
      "Training Loss: 66.5104\n",
      "Training Loss: 68.3124\n",
      "Training Loss: 65.5697\n",
      "Training Loss: 64.8447\n",
      "Training Loss: 66.6704\n",
      "Training Loss: 64.0889\n",
      "Training Loss: 66.1605\n",
      "Training Loss: 67.7859\n",
      "Training Loss: 64.8844\n",
      "Training Loss: 65.6596\n",
      "Training Loss: 67.6118\n",
      "Training Loss: 65.1444\n",
      "Training Loss: 64.9834\n",
      "Training Loss: 66.5150\n",
      "Training Loss: 63.5290\n",
      "Training Loss: 67.0677\n",
      "Training Loss: 69.0872\n",
      "Training Loss: 66.6803\n",
      "Training Loss: 63.3667\n",
      "Training Loss: 64.8484\n",
      "Training Loss: 61.8176\n",
      "Training Loss: 68.7911\n",
      "Training Loss: 70.8463\n",
      "Training Loss: 68.4717\n",
      "Training Loss: 62.1101\n",
      "Training Loss: 71.6297\n",
      "Training Loss: 76.3673\n",
      "Training Loss: 76.2668\n",
      "Training Loss: 71.8123\n",
      "Training Loss: 63.4391\n",
      "Training Loss: 71.9260\n",
      "Training Loss: 78.2387\n",
      "Training Loss: 79.6959\n",
      "Training Loss: 76.7831\n",
      "Training Loss: 69.9373\n",
      "Training Loss: 64.1168\n",
      "Training Loss: 69.2531\n",
      "Training Loss: 69.5116\n",
      "Training Loss: 65.3801\n",
      "Training Loss: 66.2605\n",
      "Training Loss: 69.1929\n",
      "Training Loss: 67.6079\n",
      "Training Loss: 61.9571\n",
      "Training Loss: 71.1347\n",
      "Training Loss: 75.2871\n",
      "Training Loss: 74.6601\n",
      "Training Loss: 69.7319\n",
      "Training Loss: 62.6844\n",
      "Training Loss: 66.2518\n",
      "Training Loss: 65.2383\n",
      "Training Loss: 63.5574\n",
      "Training Loss: 63.8920\n",
      "Training Loss: 63.7702\n",
      "Training Loss: 63.1436\n",
      "Training Loss: 65.3326\n",
      "Training Loss: 65.3486\n",
      "Training Loss: 62.6191\n",
      "Training Loss: 62.2464\n",
      "Training Loss: 66.0123\n",
      "Training Loss: 65.8193\n",
      "Training Loss: 62.3410\n",
      "Training Loss: 62.1348\n",
      "Training Loss: 65.9733\n",
      "Training Loss: 65.6432\n",
      "Training Loss: 62.6356\n",
      "Training Loss: 62.5387\n",
      "Training Loss: 65.4628\n",
      "Training Loss: 65.0427\n",
      "Training Loss: 63.3059\n",
      "Training Loss: 63.2807\n",
      "Training Loss: 64.6429\n",
      "Training Loss: 64.1639\n",
      "Training Loss: 64.2228\n",
      "Training Loss: 64.2447\n",
      "Training Loss: 63.6201\n",
      "Training Loss: 63.1023\n",
      "Training Loss: 65.3015\n",
      "Training Loss: 65.3542\n",
      "Training Loss: 62.4641\n",
      "Training Loss: 61.9209\n",
      "Training Loss: 66.4862\n",
      "Training Loss: 66.5592\n",
      "Training Loss: 62.4010\n",
      "Training Loss: 69.3178\n",
      "Training Loss: 72.2414\n",
      "Training Loss: 70.5090\n",
      "Training Loss: 64.5863\n",
      "Training Loss: 68.6271\n",
      "Training Loss: 72.9870\n",
      "Training Loss: 72.6873\n",
      "Training Loss: 68.1938\n",
      "Training Loss: 63.7366\n",
      "Training Loss: 66.9363\n",
      "Training Loss: 65.4524\n",
      "Training Loss: 63.8445\n",
      "Training Loss: 64.6671\n",
      "Training Loss: 62.4580\n",
      "Training Loss: 62.3417\n",
      "Training Loss: 64.5147\n",
      "Training Loss: 63.0615\n",
      "Training Loss: 66.1696\n",
      "Training Loss: 66.9677\n",
      "Training Loss: 63.4624\n",
      "Training Loss: 67.6416\n",
      "Training Loss: 70.0277\n",
      "Training Loss: 67.8117\n",
      "Training Loss: 62.1715\n",
      "Training Loss: 63.5774\n",
      "Training Loss: 63.0318\n",
      "Training Loss: 62.2498\n",
      "Training Loss: 64.1758\n",
      "Training Loss: 62.3336\n",
      "Training Loss: 67.2302\n",
      "Training Loss: 68.3383\n",
      "Training Loss: 65.1122\n",
      "Training Loss: 65.7092\n",
      "Training Loss: 67.8655\n",
      "Training Loss: 65.4429\n",
      "Training Loss: 64.6850\n",
      "Training Loss: 66.2557\n",
      "Training Loss: 63.4459\n",
      "Training Loss: 67.0219\n",
      "Training Loss: 68.8355\n",
      "Training Loss: 66.1044\n",
      "Training Loss: 64.3072\n",
      "Training Loss: 66.1237\n",
      "Training Loss: 63.5352\n",
      "Training Loss: 66.7287\n",
      "Training Loss: 68.3600\n",
      "Training Loss: 65.4651\n",
      "Training Loss: 65.0813\n",
      "Training Loss: 67.0284\n",
      "Training Loss: 64.5575\n",
      "Training Loss: 65.5819\n",
      "Training Loss: 67.1165\n",
      "Training Loss: 64.1344\n",
      "Training Loss: 66.4675\n",
      "Training Loss: 68.4840\n",
      "Training Loss: 66.0756\n",
      "Training Loss: 63.9816\n",
      "Training Loss: 65.4647\n",
      "Training Loss: 62.4364\n",
      "Training Loss: 68.1790\n",
      "Training Loss: 70.2324\n",
      "Training Loss: 67.8572\n",
      "Training Loss: 62.1403\n",
      "Training Loss: 63.5960\n",
      "Training Loss: 63.0677\n",
      "Training Loss: 62.0881\n",
      "Training Loss: 64.4722\n",
      "Training Loss: 62.8812\n",
      "Training Loss: 66.4808\n",
      "Training Loss: 67.2909\n",
      "Training Loss: 63.6571\n",
      "Training Loss: 67.5140\n",
      "Training Loss: 70.0499\n",
      "Training Loss: 68.1091\n",
      "Training Loss: 62.1393\n",
      "Training Loss: 71.2405\n",
      "Training Loss: 75.6557\n",
      "Training Loss: 75.2665\n",
      "Training Loss: 70.5532\n",
      "Training Loss: 61.9485\n",
      "Training Loss: 73.5963\n",
      "Training Loss: 80.0936\n",
      "Training Loss: 81.7181\n",
      "Training Loss: 78.9572\n",
      "Training Loss: 72.2494\n",
      "Training Loss: 61.9891\n",
      "Training Loss: 75.3178\n",
      "Training Loss: 83.2653\n",
      "Training Loss: 86.0551\n",
      "Training Loss: 84.2031\n",
      "Training Loss: 78.1735\n",
      "Training Loss: 68.3842\n",
      "Training Loss: 68.3134\n",
      "Training Loss: 75.7547\n",
      "Training Loss: 78.2291\n",
      "Training Loss: 76.2330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 70.2136\n",
      "Training Loss: 63.0786\n",
      "Training Loss: 67.5347\n",
      "Training Loss: 67.1823\n",
      "Training Loss: 62.5025\n",
      "Training Loss: 69.5758\n",
      "Training Loss: 72.9453\n",
      "Training Loss: 71.7551\n",
      "Training Loss: 66.4609\n",
      "Training Loss: 66.2295\n",
      "Training Loss: 70.0884\n",
      "Training Loss: 69.1988\n",
      "Training Loss: 64.0355\n",
      "Training Loss: 68.4956\n",
      "Training Loss: 72.2505\n",
      "Training Loss: 71.4070\n",
      "Training Loss: 66.4251\n",
      "Training Loss: 65.9802\n",
      "Training Loss: 69.5821\n",
      "Training Loss: 68.4612\n",
      "Training Loss: 63.0898\n",
      "Training Loss: 69.6102\n",
      "Training Loss: 73.5309\n",
      "Training Loss: 72.8367\n",
      "Training Loss: 67.9892\n",
      "Training Loss: 64.2673\n",
      "Training Loss: 67.7585\n",
      "Training Loss: 66.5381\n",
      "Training Loss: 62.5422\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-bee60c324b71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training complete in {:.0f}m {:.0f}s'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_elapsed\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_elapsed\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Finished training'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-bee60c324b71>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtrain_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = nn.L1Loss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 1\n",
    "epochs_no_improve_limit = 7\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "def train():\n",
    "    epochs_no_improve = 0\n",
    "    min_val_loss = np.Inf\n",
    "    since = time.time()\n",
    "    iteration = 0\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, epochs))\n",
    "        print('-' * 10)\n",
    "        val_loss = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        for train_inputs, train_labels in Generator(training_file, batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            output = net(torch.from_numpy(train_inputs.transpose(0,3,1,2)).to(device))\n",
    "            loss = loss_fn(output[0].cpu(), torch.from_numpy(train_labels).cpu())\n",
    "            train_loss += loss\n",
    "            \n",
    "#             fig = plt.figure(figsize = (32,24))\n",
    "#             ax1 = fig.add_subplot(2, 2, 1)\n",
    "#             ax1.set_title('Input red')\n",
    "#             ax2 = fig.add_subplot(2, 2, 2)\n",
    "#             ax2.set_title('Input green')\n",
    "#             ax3 = fig.add_subplot(2, 2, 3)\n",
    "#             ax3.set_title('Label')\n",
    "#             ax4 = fig.add_subplot(2, 2, 4)\n",
    "#             ax4.set_title('Output of the CNN')\n",
    "\n",
    "#             sns.heatmap(train_inputs[0][0], vmin=0, vmax=1, cmap='gray', ax=ax1, cbar_kws={'shrink': .9})\n",
    "#             sns.heatmap(train_inputs[0][1], vmin=0, vmax=1, ax=ax2, cbar_kws={'shrink': .9})\n",
    "#             sns.heatmap(train_labels[0][0], vmin=0, vmax=1, ax=ax3, cbar_kws={'shrink': .9})\n",
    "#             sns.heatmap(output.cpu().detach().numpy()[0], vmin=0, vmax=1, ax=ax4, cbar_kws={'shrink': .9})\n",
    "#             plt.show()\n",
    "\n",
    "\n",
    "#             writer.add_scalar('Loss/train', loss, iteration)\n",
    "\n",
    "            print('Training Loss: {:.4f}'.format(loss.item()))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            iteration += 1\n",
    "        \n",
    "        train_loss = train_loss / len(dataloaders['train'])\n",
    "        train_losses.append(float(train_loss))\n",
    "        \n",
    "        del train_inputs\n",
    "        del train_labels\n",
    "        del output\n",
    "        del loss\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for val_inputs, val_labels in dataloaders['val']:\n",
    "#                 torch.cuda.empty_cache()\n",
    "#                 output = net(val_inputs.to(device)).reshape(batch_size, output_height, output_width)\n",
    "#                 loss = loss_fn(output.cpu(), val_labels.cpu().reshape(batch_size, output_height, output_width))\n",
    "#                 val_loss += loss\n",
    "\n",
    "#             val_loss = val_loss / len(dataloaders['val'])\n",
    "#             val_losses.append(float(val_loss))\n",
    "#             print('-' * 10)\n",
    "#             print('Validation Loss: {:.4f}'.format(val_loss))\n",
    "\n",
    "#             if val_loss < min_val_loss:\n",
    "#                 torch.save({'state_dict': net.state_dict()}, 'Nets/pt-labi_CNN.pt')\n",
    "#                 epochs_no_improve = 0\n",
    "#                 min_val_loss = val_loss\n",
    "#             else:\n",
    "#                 epochs_no_improve += 1\n",
    "#                 if epochs_no_improve == epochs_no_improve_limit:\n",
    "#                     print('Early stopping initiated')\n",
    "#                     model = torch.load('Nets/pt-labi_CNN.pt')\n",
    "#                     print('Best model so far has been loaded')\n",
    "    print('Least validation Loss: {:4f}'.format(min_val_loss))\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Finished training')\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'testing.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-72d74249f14f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Average test loss: '\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Testing completed'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-38-72d74249f14f>\u001b[0m in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mnum_test_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtest_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mnum_test_samples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\elias\\Documents\\Programming\\A.I\\CNN_ER_FA\\BatchMaker.py\u001b[0m in \u001b[0;36mBatchMaker\u001b[1;34m(images_path, batch_size)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m    \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m    \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m       \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m       \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'testing.csv'"
     ]
    }
   ],
   "source": [
    "output_width = 240\n",
    "output_height = 180\n",
    "def test():\n",
    "    test_loss = 0\n",
    "    num_test_samples = 10\n",
    "    with torch.no_grad():\n",
    "        for idx, (test_inputs, test_labels) in enumerate(Generator(testing_file, 1)):\n",
    "            if idx >= num_test_samples:\n",
    "                break\n",
    "            else:\n",
    "                output = net(torch.from_numpy(test_inputs.transpose(0,3,1,2)).to(device))\n",
    "                loss = loss_fn(output[0].cpu(), torch.from_numpy(test_labels).cpu())\n",
    "                output = output[0].reshape(output_height, output_width).cpu().detach().numpy()\n",
    "                test_labels = test_labels[0].reshape(output_height, output_width)\n",
    "                test_loss += loss\n",
    "\n",
    "                fig = plt.figure(figsize = (32,24))\n",
    "                ax1 = fig.add_subplot(2, 2, 1)\n",
    "                ax1.set_title('Output')\n",
    "                ax2 = fig.add_subplot(2, 2, 2)\n",
    "                ax2.set_title('Label')\n",
    "\n",
    "                sns.heatmap(output, vmin=0, vmax=1, ax=ax1, square=True, cbar_kws={'shrink': .8})\n",
    "                sns.heatmap(test_labels, vmin=0, vmax=1, ax=ax2, square=True, cbar_kws={'shrink': .8})\n",
    "                plt.show()\n",
    "        \n",
    "        test_loss = test_loss/num_test_samples\n",
    "        print('Average test loss: ' ,test_loss.numpy())\n",
    "        print('Testing completed')\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
