{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.5.0\n",
      "Torchvision Version:  0.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.optim as optim\n",
    "import BatchMaker\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import math\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64))\n",
    "    \n",
    "        self.layer2 = nn.Sequential( \n",
    "            nn.Conv2d(64, 64,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.MaxPool2d(2, stride=2, padding=0))\n",
    "            \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64,128,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128))\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128))\n",
    "            \n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.MaxPool2d(2, stride=2, padding=0))\n",
    "            \n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256))\n",
    "            \n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256))\n",
    "            \n",
    "        self.layer9 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256))\n",
    "            \n",
    "        self.layer10 = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "            \n",
    "        self.layer11 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128))\n",
    "            \n",
    "        self.layer12 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128))\n",
    "            \n",
    "        self.layer13 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128))\n",
    "            \n",
    "        self.layer14 = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "            \n",
    "        self.layer15 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64))\n",
    "            \n",
    "        self.layer16 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64))\n",
    "            \n",
    "        self.layer17 = nn.Sequential(\n",
    "            nn.Conv2d(64, 1,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(1))\n",
    "        \n",
    "        self.layer18 = nn.Softmax(dim=1)\n",
    "            \n",
    "            \n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)  \n",
    "        x = self.layer2(x)  \n",
    "        x = self.layer3(x)  \n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)  \n",
    "        x = self.layer7(x)\n",
    "        x = self.layer8(x)\n",
    "        x = self.layer9(x)  \n",
    "        x = self.layer10(x)  \n",
    "        x = self.layer11(x)  \n",
    "        x = self.layer12(x)  \n",
    "        x = self.layer13(x)  \n",
    "        x = self.layer14(x)  \n",
    "        x = self.layer15(x)  \n",
    "        x = self.layer16(x)\n",
    "        x = self.layer17(x) \n",
    "        x = x.view(1, -1)\n",
    "        x = self.layer18(x)\n",
    "        x = x.reshape(1, 180, 240)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on your GeForce GTX 1080 (GPU)\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and move the model over to GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda: 0\")\n",
    "    gpu_name = torch.cuda.get_device_name()\n",
    "    print(f\"Running on your {gpu_name} (GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on your CPU\")\n",
    "\n",
    "net = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generator = BatchMaker.BatchMaker\n",
    "\n",
    "batch_size = 1\n",
    "training_file = \"training.csv\"\n",
    "testing_file = \"testing.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(output, target):\n",
    "    \n",
    "    center_output = np.where(output==np.amax(output))\n",
    "    center_target = np.where(target==np.amax(target))\n",
    "    \n",
    "    x1 = center_output[1]\n",
    "    y1 = center_output[0]\n",
    "    x2 = center_target[1]\n",
    "    y2 = center_target[0]\n",
    "    \n",
    "    distance = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "----------\n",
      "Training Loss: 19.9700\n",
      "Training Loss: 45.8272\n",
      "Training Loss: 6.9661\n",
      "Training Loss: 18.3688\n",
      "Training Loss: 51.0700\n",
      "Training Loss: 1.7097\n",
      "Training Loss: 0.4971\n",
      "Training Loss: 0.9871\n",
      "Training Loss: 1.1474\n",
      "Training Loss: 11.4927\n",
      "Training Loss: 1.4604\n",
      "Training Loss: 1.5524\n",
      "Training Loss: 34.9468\n",
      "Training Loss: 0.4021\n",
      "Training Loss: 19.4339\n",
      "Training Loss: 8.6957\n",
      "Training Loss: 0.9223\n",
      "Training Loss: 35.3130\n",
      "Training Loss: 5.9800\n",
      "Training Loss: 31.6537\n",
      "Training Loss: 3.3916\n",
      "Training Loss: 5.4231\n",
      "Training Loss: 32.2879\n",
      "Training Loss: 2.3258\n",
      "Training Loss: 17.9514\n",
      "Training Loss: 23.9330\n",
      "Training Loss: 47.2057\n",
      "Training Loss: 2.7763\n",
      "Training Loss: 3.3919\n",
      "Training Loss: 50.7545\n",
      "Training Loss: 2.8460\n",
      "Training Loss: 58.9500\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.8997\n",
      "Training Loss: 1.2817\n",
      "Training Loss: 9.2436\n",
      "Training Loss: 30.8862\n",
      "Training Loss: 58.7325\n",
      "Training Loss: 0.2532\n",
      "Training Loss: 0.2364\n",
      "Training Loss: 2.2940\n",
      "Training Loss: 0.4636\n",
      "Training Loss: 0.3978\n",
      "Training Loss: 1.7669\n",
      "Training Loss: 1.0587\n",
      "Training Loss: 0.4429\n",
      "Training Loss: 57.7376\n",
      "Training Loss: 1.7999\n",
      "Training Loss: 1.4930\n",
      "Training Loss: 63.3098\n",
      "Training Loss: 1.0080\n",
      "Training Loss: 1.9327\n",
      "Training Loss: 2.0462\n",
      "Training Loss: 3.9189\n",
      "Training Loss: 1.4974\n",
      "Training Loss: 0.4923\n",
      "Training Loss: 0.4938\n",
      "Training Loss: 15.4795\n",
      "Training Loss: 0.5129\n",
      "Training Loss: 44.7656\n",
      "Training Loss: 0.4190\n",
      "Training Loss: 12.9582\n",
      "Training Loss: 36.8825\n",
      "Training Loss: 32.9271\n",
      "Training Loss: 3.4483\n",
      "Training Loss: 48.1220\n",
      "Training Loss: 0.4578\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 19.8003\n",
      "Training Loss: 4.9629\n",
      "Training Loss: 6.9731\n",
      "Training Loss: 11.9666\n",
      "Training Loss: 33.6684\n",
      "Training Loss: 40.9220\n",
      "Training Loss: 3.3814\n",
      "Training Loss: 2.6904\n",
      "Training Loss: 38.4346\n",
      "Training Loss: 22.8675\n",
      "Training Loss: 0.4296\n",
      "Training Loss: 26.4359\n",
      "Training Loss: 1.7664\n",
      "Training Loss: 3.4925\n",
      "Training Loss: 22.9977\n",
      "Training Loss: 0.4697\n",
      "Training Loss: 11.4680\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 27.4865\n",
      "Training Loss: 7.8542\n",
      "Training Loss: 45.8357\n",
      "Training Loss: 0.4077\n",
      "Training Loss: 1.8168\n",
      "Training Loss: 30.4853\n",
      "Training Loss: 1.6033\n",
      "Training Loss: 70.9903\n",
      "Training Loss: 20.4903\n",
      "Training Loss: 2.4945\n",
      "Training Loss: 39.3276\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.2597\n",
      "Training Loss: 46.8619\n",
      "Training Loss: 1.4815\n",
      "Training Loss: 23.7826\n",
      "Training Loss: 6.4930\n",
      "Training Loss: 2.4822\n",
      "Training Loss: 5.8528\n",
      "Training Loss: 18.4427\n",
      "Training Loss: 11.5527\n",
      "Training Loss: 50.0201\n",
      "Training Loss: 9.4266\n",
      "Training Loss: 34.9575\n",
      "Training Loss: 13.9532\n",
      "Training Loss: 32.8531\n",
      "Training Loss: 31.4431\n",
      "Training Loss: 24.3094\n",
      "Training Loss: 21.8555\n",
      "Training Loss: 9.9615\n",
      "Training Loss: 5.9885\n",
      "Training Loss: 1.4530\n",
      "Training Loss: 1.4784\n",
      "Training Loss: 2.3734\n",
      "Training Loss: 38.3552\n",
      "Training Loss: 22.4588\n",
      "Training Loss: 32.3816\n",
      "Training Loss: 0.9496\n",
      "Training Loss: 18.8491\n",
      "Training Loss: 73.6074\n",
      "Training Loss: 23.9662\n",
      "Training Loss: 0.4778\n",
      "Training Loss: 1.0474\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 21.0692\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1395\n",
      "Training Loss: 18.9341\n",
      "Training Loss: 0.6050\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.4258\n",
      "Training Loss: 0.9055\n",
      "Training Loss: 0.1721\n",
      "Training Loss: 0.3294\n",
      "Training Loss: 1.7140\n",
      "Training Loss: 0.4397\n",
      "Training Loss: 1.4962\n",
      "Training Loss: 21.4727\n",
      "Training Loss: 3.2067\n",
      "Training Loss: 0.1851\n",
      "Training Loss: 0.7499\n",
      "Training Loss: 8.4751\n",
      "Training Loss: 35.4678\n",
      "Training Loss: 1.6596\n",
      "Training Loss: 3.3304\n",
      "Training Loss: 0.4242\n",
      "Training Loss: 1.1163\n",
      "Training Loss: 1.5460\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.7038\n",
      "Training Loss: 0.3545\n",
      "Training Loss: 0.3479\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.4725\n",
      "Training Loss: 25.3069\n",
      "Training Loss: 25.6477\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 74.3899\n",
      "Training Loss: 65.4137\n",
      "Training Loss: 20.7569\n",
      "Training Loss: 2.6335\n",
      "Training Loss: 21.8229\n",
      "Training Loss: 3.9998\n",
      "Training Loss: 1.3915\n",
      "Training Loss: 42.3030\n",
      "Training Loss: 3.3646\n",
      "Training Loss: 53.9089\n",
      "Training Loss: 26.9676\n",
      "Training Loss: 1.4950\n",
      "Training Loss: 0.8926\n",
      "Training Loss: 4.8160\n",
      "Training Loss: 0.3271\n",
      "Training Loss: 0.7250\n",
      "Training Loss: 7.4965\n",
      "Training Loss: 27.3690\n",
      "Training Loss: 0.2356\n",
      "Training Loss: 1.7296\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 47.8105\n",
      "Training Loss: 1.6410\n",
      "Training Loss: 2.4977\n",
      "Training Loss: 0.4993\n",
      "Training Loss: 3.9081\n",
      "Training Loss: 4.4999\n",
      "Training Loss: 0.4996\n",
      "Training Loss: 11.6388\n",
      "Training Loss: 1.0876\n",
      "Training Loss: 30.9174\n",
      "Training Loss: 0.1896\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.5659\n",
      "Training Loss: 0.7519\n",
      "Training Loss: 0.3889\n",
      "Training Loss: 0.4094\n",
      "Training Loss: 0.7048\n",
      "Training Loss: 53.6482\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.7149\n",
      "Training Loss: 1.7181\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 21.8177\n",
      "Training Loss: 0.7091\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 8.3305\n",
      "Training Loss: 2.4970\n",
      "Training Loss: 6.9990\n",
      "Training Loss: 0.3486\n",
      "Training Loss: 1.5000\n",
      "Training Loss: 0.2500\n",
      "Training Loss: 1.9999\n",
      "Training Loss: 9.4998\n",
      "Training Loss: 12.0130\n",
      "Training Loss: 2.9014\n",
      "Training Loss: 0.4202\n",
      "Training Loss: 8.3287\n",
      "Training Loss: 4.6342\n",
      "Training Loss: 3.4132\n",
      "Training Loss: 49.1717\n",
      "Training Loss: 1.9633\n",
      "Training Loss: 0.5787\n",
      "Training Loss: 1.4877\n",
      "Training Loss: 0.8212\n",
      "Training Loss: 47.4394\n",
      "Training Loss: 2.2615\n",
      "Training Loss: 1.8457\n",
      "Training Loss: 22.5668\n",
      "Training Loss: 0.4526\n",
      "Training Loss: 0.8582\n",
      "Training Loss: 0.3621\n",
      "Training Loss: 46.8497\n",
      "Training Loss: 0.7134\n",
      "Training Loss: 1.6320\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 14.0928\n",
      "Training Loss: 0.1703\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1002\n",
      "Training Loss: 1.2822\n",
      "Training Loss: 0.4998\n",
      "Training Loss: 33.9768\n",
      "Training Loss: 1.3446\n",
      "Training Loss: 0.2126\n",
      "Training Loss: 2.0334\n",
      "Training Loss: 32.9902\n",
      "Training Loss: 9.9941\n",
      "Training Loss: 0.9221\n",
      "Training Loss: 21.9424\n",
      "Training Loss: 3.8950\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 4.3301\n",
      "Training Loss: 5.7251\n",
      "Training Loss: 0.2073\n",
      "Training Loss: 0.8484\n",
      "Training Loss: 8.3050\n",
      "Training Loss: 11.8442\n",
      "Training Loss: 0.4258\n",
      "Training Loss: 1.4123\n",
      "Training Loss: 1.7199\n",
      "Training Loss: 0.9142\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1574\n",
      "Training Loss: 1.3941\n",
      "Training Loss: 1.2705\n",
      "Training Loss: 0.6872\n",
      "Training Loss: 0.7770\n",
      "Training Loss: 0.8388\n",
      "Training Loss: 2.8585\n",
      "Training Loss: 1.4064\n",
      "Training Loss: 0.2727\n",
      "Training Loss: 1.7245\n",
      "Training Loss: 64.5445\n",
      "Training Loss: 2.0882\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.1431\n",
      "Training Loss: 1.8893\n",
      "Training Loss: 2.5810\n",
      "Training Loss: 3.8262\n",
      "Training Loss: 2.8572\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 41.4126\n",
      "Training Loss: 20.9589\n",
      "Training Loss: 0.1539\n",
      "Training Loss: 0.0978\n",
      "Training Loss: 1.0858\n",
      "Training Loss: 0.8990\n",
      "Training Loss: 0.2826\n",
      "Training Loss: 34.2622\n",
      "Training Loss: 6.3996\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 28.4292\n",
      "Training Loss: 0.9926\n",
      "Training Loss: 0.4036\n",
      "Training Loss: 25.2168\n",
      "Training Loss: 0.4898\n",
      "Training Loss: 0.3190\n",
      "Training Loss: 1.2561\n",
      "Training Loss: 51.0781\n",
      "Training Loss: 0.5050\n",
      "Training Loss: 21.5202\n",
      "Training Loss: 0.3776\n",
      "Training Loss: 0.4566\n",
      "Training Loss: 14.2105\n",
      "Training Loss: 2.1129\n",
      "Training Loss: 13.9296\n",
      "Training Loss: 33.1872\n",
      "Training Loss: 0.8374\n",
      "Training Loss: 1.5308\n",
      "Training Loss: 30.9890\n",
      "Training Loss: 1.9759\n",
      "Training Loss: 1.2428\n",
      "Training Loss: 43.1630\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.6308\n",
      "Training Loss: 0.5135\n",
      "Training Loss: 46.7613\n",
      "Training Loss: 0.3278\n",
      "Training Loss: 48.7463\n",
      "Training Loss: 0.3806\n",
      "Training Loss: 0.3445\n",
      "Training Loss: 4.8781\n",
      "Training Loss: 3.1700\n",
      "Training Loss: 61.4612\n",
      "Training Loss: 2.5681\n",
      "Training Loss: 0.7449\n",
      "Training Loss: 1.4179\n",
      "Training Loss: 4.2956\n",
      "Training Loss: 0.9795\n",
      "Training Loss: 0.3345\n",
      "Training Loss: 0.3214\n",
      "Training Loss: 5.8678\n",
      "Training Loss: 1.9821\n",
      "Training Loss: 70.1798\n",
      "Training Loss: 0.2139\n",
      "Training Loss: 38.3640\n",
      "Training Loss: 0.8459\n",
      "Training Loss: 0.4776\n",
      "Training Loss: 0.5958\n",
      "Training Loss: 28.9860\n",
      "Training Loss: 0.8346\n",
      "Training Loss: 61.3892\n",
      "Training Loss: 3.9958\n",
      "Training Loss: 2.8856\n",
      "Training Loss: 52.2004\n",
      "Training Loss: 59.3368\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 55.1625\n",
      "Training Loss: 1.5488\n",
      "Training Loss: 0.2743\n",
      "Training Loss: 0.9235\n",
      "Training Loss: 0.6419\n",
      "Training Loss: 0.8737\n",
      "Training Loss: 1.5046\n",
      "Training Loss: 0.5194\n",
      "Training Loss: 0.9620\n",
      "Training Loss: 1.4118\n",
      "Training Loss: 3.2508\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 2.0693\n",
      "Training Loss: 23.7794\n",
      "Training Loss: 4.3033\n",
      "Training Loss: 0.5620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6297\n",
      "Training Loss: 42.4651\n",
      "Training Loss: 0.6955\n",
      "Training Loss: 1.8484\n",
      "Training Loss: 63.3979\n",
      "Training Loss: 49.4697\n",
      "Training Loss: 1.2152\n",
      "Training Loss: 1.0779\n",
      "Training Loss: 0.6477\n",
      "Training Loss: 0.6779\n",
      "Training Loss: 1.8988\n",
      "Training Loss: 53.7423\n",
      "Training Loss: 0.2021\n",
      "Training Loss: 0.2711\n",
      "Training Loss: 0.9633\n",
      "Training Loss: 0.3382\n",
      "Training Loss: 0.2598\n",
      "Training Loss: 0.7020\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.8247\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 2.9669\n",
      "Training Loss: 17.9387\n",
      "Training Loss: 7.3468\n",
      "Training Loss: 0.3833\n",
      "Training Loss: 0.3890\n",
      "Training Loss: 21.8923\n",
      "Training Loss: 0.1255\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.3420\n",
      "Training Loss: 26.6629\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.4190\n",
      "Training Loss: 11.4013\n",
      "Training Loss: 0.4811\n",
      "Training Loss: 0.4072\n",
      "Training Loss: 1.0852\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 50.8197\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2281\n",
      "Training Loss: 0.6837\n",
      "Training Loss: 7.8994\n",
      "Training Loss: 1.0637\n",
      "Training Loss: 0.8339\n",
      "Training Loss: 1.7709\n",
      "Training Loss: 3.3484\n",
      "Training Loss: 45.6165\n",
      "Training Loss: 12.0662\n",
      "Training Loss: 1.4016\n",
      "Training Loss: 1.5032\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.7881\n",
      "Training Loss: 2.3037\n",
      "Training Loss: 0.6959\n",
      "Training Loss: 0.4846\n",
      "Training Loss: 0.9978\n",
      "Training Loss: 31.2121\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 2.1355\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.9051\n",
      "Training Loss: 48.6354\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.9305\n",
      "Training Loss: 2.7851\n",
      "Training Loss: 0.2345\n",
      "Training Loss: 0.6511\n",
      "Training Loss: 7.9437\n",
      "Training Loss: 20.8242\n",
      "Training Loss: 0.7991\n",
      "Training Loss: 1.3642\n",
      "Training Loss: 3.2351\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.6507\n",
      "Training Loss: 1.1769\n",
      "Training Loss: 0.1659\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2719\n",
      "Training Loss: 2.2519\n",
      "Training Loss: 0.1145\n",
      "Training Loss: 10.6841\n",
      "Training Loss: 42.4813\n",
      "Training Loss: 2.2384\n",
      "Training Loss: 38.6096\n",
      "Training Loss: 1.4432\n",
      "Training Loss: 0.4032\n",
      "Training Loss: 2.0896\n",
      "Training Loss: 0.9625\n",
      "Training Loss: 33.6352\n",
      "Training Loss: 13.1115\n",
      "Training Loss: 0.7395\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 51.4617\n",
      "Training Loss: 66.9907\n",
      "Training Loss: 21.7540\n",
      "Training Loss: 53.6978\n",
      "Training Loss: 2.9971\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.3886\n",
      "Training Loss: 1.4229\n",
      "Training Loss: 0.9817\n",
      "Training Loss: 61.2179\n",
      "Training Loss: 15.7267\n",
      "Training Loss: 49.1101\n",
      "Training Loss: 0.7033\n",
      "Training Loss: 10.8320\n",
      "Training Loss: 0.9508\n",
      "Training Loss: 1.7964\n",
      "Training Loss: 0.7981\n",
      "Training Loss: 22.2276\n",
      "Training Loss: 75.8623\n",
      "Training Loss: 4.9570\n",
      "Training Loss: 1.2294\n",
      "Training Loss: 1.9846\n",
      "Training Loss: 9.8645\n",
      "Training Loss: 17.6039\n",
      "Training Loss: 25.8107\n",
      "Training Loss: 2.9180\n",
      "Training Loss: 38.4761\n",
      "Training Loss: 2.4926\n",
      "Training Loss: 1.3338\n",
      "Training Loss: 0.8545\n",
      "Training Loss: 47.0507\n",
      "Training Loss: 27.1997\n",
      "Training Loss: 15.2898\n",
      "Training Loss: 4.2587\n",
      "Training Loss: 2.2156\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2168\n",
      "Training Loss: 8.5734\n",
      "Training Loss: 0.4236\n",
      "Training Loss: 3.7170\n",
      "Training Loss: 12.4119\n",
      "Training Loss: 0.4386\n",
      "Training Loss: 0.8806\n",
      "Training Loss: 32.3349\n",
      "Training Loss: 2.9564\n",
      "Training Loss: 1.4084\n",
      "Training Loss: 3.8535\n",
      "Training Loss: 0.4879\n",
      "Training Loss: 0.9105\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 2.6663\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 34.6045\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.9050\n",
      "Training Loss: 3.9602\n",
      "Training Loss: 53.7109\n",
      "Training Loss: 3.8050\n",
      "Training Loss: 23.8158\n",
      "Training Loss: 1.1648\n",
      "Training Loss: 0.2173\n",
      "Training Loss: 19.4023\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.5302\n",
      "Training Loss: 1.4430\n",
      "Training Loss: 0.2057\n",
      "Training Loss: 29.7374\n",
      "Training Loss: 1.1470\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.7194\n",
      "Training Loss: 0.7952\n",
      "Training Loss: 0.9332\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 27.5043\n",
      "Training Loss: 6.3781\n",
      "Training Loss: 1.3341\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 2.3306\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.5336\n",
      "Training Loss: 0.8944\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.3985\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 8.9889\n",
      "Training Loss: 0.1707\n",
      "Training Loss: 0.2045\n",
      "Training Loss: 0.4132\n",
      "Training Loss: 56.2275\n",
      "Training Loss: 43.2017\n",
      "Training Loss: 0.3309\n",
      "Training Loss: 45.9215\n",
      "Training Loss: 2.7854\n",
      "Training Loss: 0.7064\n",
      "Training Loss: 0.6558\n",
      "Training Loss: 0.1139\n",
      "Training Loss: 0.6124\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.8092\n",
      "Training Loss: 75.6949\n",
      "Training Loss: 4.1176\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.6220\n",
      "Training Loss: 0.4945\n",
      "Training Loss: 0.9622\n",
      "Training Loss: 0.4189\n",
      "Training Loss: 1.3852\n",
      "Training Loss: 0.3208\n",
      "Training Loss: 0.2476\n",
      "Training Loss: 0.2916\n",
      "Training Loss: 73.6999\n",
      "Training Loss: 0.4089\n",
      "Training Loss: 0.3196\n",
      "Training Loss: 0.5136\n",
      "Training Loss: 0.9534\n",
      "Training Loss: 0.3085\n",
      "Training Loss: 0.8196\n",
      "Training Loss: 2.7790\n",
      "Training Loss: 0.4933\n",
      "Training Loss: 1.0351\n",
      "Training Loss: 22.0620\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1542\n",
      "Training Loss: 0.7183\n",
      "Training Loss: 0.9639\n",
      "Training Loss: 1.5741\n",
      "Training Loss: 1.7057\n",
      "Training Loss: 0.1362\n",
      "Training Loss: 1.0312\n",
      "Training Loss: 0.7036\n",
      "Training Loss: 0.3735\n",
      "Training Loss: 0.4593\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1740\n",
      "Training Loss: 0.0956\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.9260\n",
      "Training Loss: 2.6053\n",
      "Training Loss: 2.1103\n",
      "Training Loss: 0.1456\n",
      "Training Loss: 0.2247\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2019\n",
      "Training Loss: 0.3268\n",
      "Training Loss: 2.0029\n",
      "Training Loss: 0.2416\n",
      "Training Loss: 0.3023\n",
      "Training Loss: 0.2116\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2478\n",
      "Training Loss: 0.3883\n",
      "Training Loss: 0.7927\n",
      "Training Loss: 2.2856\n",
      "Training Loss: 1.3868\n",
      "Training Loss: 0.7693\n",
      "Training Loss: 0.5341\n",
      "Training Loss: 2.7072\n",
      "Training Loss: 0.6238\n",
      "Training Loss: 0.2511\n",
      "Training Loss: 4.7666\n",
      "Training Loss: 31.4066\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.3171\n",
      "Training Loss: 0.6320\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1735\n",
      "Training Loss: 2.8007\n",
      "Training Loss: 0.3965\n",
      "Training Loss: 0.5404\n",
      "Training Loss: 0.7843\n",
      "Training Loss: 0.3377\n",
      "Training Loss: 0.6825\n",
      "Training Loss: 0.6054\n",
      "Training Loss: 0.4933\n",
      "Training Loss: 0.2276\n",
      "Training Loss: 1.2377\n",
      "Training Loss: 0.4307\n",
      "Training Loss: 0.3551\n",
      "Training Loss: 0.1866\n",
      "Training Loss: 0.1710\n",
      "Training Loss: 0.6859\n",
      "Training Loss: 0.8336\n",
      "Training Loss: 1.2607\n",
      "Training Loss: 0.1116\n",
      "Training Loss: 0.6913\n",
      "Training Loss: 0.1210\n",
      "Training Loss: 0.3590\n",
      "Training Loss: 0.2739\n",
      "Training Loss: 0.1273\n",
      "Training Loss: 3.0098\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 2.9031\n",
      "Training Loss: 0.2765\n",
      "Training Loss: 0.4991\n",
      "Training Loss: 0.5928\n",
      "Training Loss: 0.4561\n",
      "Training Loss: 0.3896\n",
      "Training Loss: 0.7664\n",
      "Training Loss: 0.1045\n",
      "Training Loss: 0.2708\n",
      "Training Loss: 38.4003\n",
      "Training Loss: 0.2247\n",
      "Training Loss: 18.2149\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.7118\n",
      "Training Loss: 3.0075\n",
      "Training Loss: 1.6692\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 47.4529\n",
      "Training Loss: 0.2315\n",
      "Training Loss: 0.5612\n",
      "Training Loss: 1.0636\n",
      "Training Loss: 1.0756\n",
      "Training Loss: 0.3299\n",
      "Training Loss: 42.6641\n",
      "Training Loss: 0.3773\n",
      "Training Loss: 2.0822\n",
      "Training Loss: 22.9878\n",
      "Training Loss: 1.0531\n",
      "Training Loss: 2.6332\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 2.4717\n",
      "Training Loss: 0.6083\n",
      "Training Loss: 0.4942\n",
      "Training Loss: 0.3676\n",
      "Training Loss: 0.1451\n",
      "Training Loss: 0.3546\n",
      "Training Loss: 2.5401\n",
      "Training Loss: 1.8543\n",
      "Training Loss: 0.3128\n",
      "Training Loss: 1.0709\n",
      "Training Loss: 1.4359\n",
      "Training Loss: 2.7276\n",
      "Training Loss: 0.2136\n",
      "Training Loss: 0.4230\n",
      "Training Loss: 1.0181\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2603\n",
      "Training Loss: 1.5183\n",
      "Training Loss: 1.3212\n",
      "Training Loss: 0.6642\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.5606\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.7863\n",
      "Training Loss: 0.3097\n",
      "Training Loss: 0.6022\n",
      "Training Loss: 0.1351\n",
      "Training Loss: 0.3400\n",
      "Training Loss: 0.3270\n",
      "Training Loss: 0.1546\n",
      "Training Loss: 11.9794\n",
      "Training Loss: 0.4157\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.4197\n",
      "Training Loss: 4.4158\n",
      "Training Loss: 0.7330\n",
      "Training Loss: 0.1522\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.8718\n",
      "Training Loss: 31.6742\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2424\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.8867\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2806\n",
      "Training Loss: 0.3893\n",
      "Training Loss: 0.1887\n",
      "Training Loss: 2.4985\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1755\n",
      "Training Loss: 3.5619\n",
      "Training Loss: 1.3688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4301\n",
      "Training Loss: 1.1248\n",
      "Training Loss: 1.5541\n",
      "Training Loss: 5.7707\n",
      "Training Loss: 14.4839\n",
      "Training Loss: 0.3870\n",
      "Training Loss: 0.6786\n",
      "Training Loss: 0.3342\n",
      "Training Loss: 5.7557\n",
      "Training Loss: 0.4022\n",
      "Training Loss: 0.7321\n",
      "Training Loss: 0.8134\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.5789\n",
      "Training Loss: 0.6464\n",
      "Training Loss: 0.0824\n",
      "Training Loss: 0.1321\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.4272\n",
      "Training Loss: 0.0935\n",
      "Training Loss: 2.5665\n",
      "Training Loss: 0.3112\n",
      "Training Loss: 0.9950\n",
      "Training Loss: 1.3359\n",
      "Training Loss: 0.5797\n",
      "Training Loss: 0.2151\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.3557\n",
      "Training Loss: 0.2356\n",
      "Training Loss: 0.3030\n",
      "Training Loss: 0.2742\n",
      "Training Loss: 0.3292\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.9107\n",
      "Training Loss: 3.0504\n",
      "Training Loss: 0.9613\n",
      "Training Loss: 0.1234\n",
      "Training Loss: 0.0994\n",
      "Training Loss: 0.1497\n",
      "Training Loss: 16.7392\n",
      "Training Loss: 0.2848\n",
      "Training Loss: 0.3478\n",
      "Training Loss: 1.1206\n",
      "Training Loss: 0.4684\n",
      "Training Loss: 0.2388\n",
      "Training Loss: 25.9954\n",
      "Training Loss: 0.4971\n",
      "Training Loss: 0.7782\n",
      "Training Loss: 1.0735\n",
      "Training Loss: 20.9680\n",
      "Training Loss: 0.1844\n",
      "Training Loss: 0.2009\n",
      "Training Loss: 0.8001\n",
      "Training Loss: 38.8970\n",
      "Training Loss: 72.2310\n",
      "Training Loss: 0.7260\n",
      "Training Loss: 0.6401\n",
      "Training Loss: 0.5877\n",
      "Training Loss: 2.2746\n",
      "Training Loss: 11.3303\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 2.6923\n",
      "Training Loss: 0.6509\n",
      "Training Loss: 0.5657\n",
      "Training Loss: 0.1831\n",
      "Training Loss: 1.3407\n",
      "Training Loss: 0.2844\n",
      "Training Loss: 0.7149\n",
      "Training Loss: 0.3796\n",
      "Training Loss: 2.4967\n",
      "Training Loss: 0.3431\n",
      "Training Loss: 0.3347\n",
      "Training Loss: 0.4590\n",
      "Training Loss: 0.3332\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.3253\n",
      "Training Loss: 0.3123\n",
      "Training Loss: 0.8551\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.1435\n",
      "Training Loss: 2.7970\n",
      "Training Loss: 0.3824\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.3870\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.0984\n",
      "Training Loss: 15.9591\n",
      "Training Loss: 0.9276\n",
      "Training Loss: 0.0941\n",
      "Training Loss: 60.9318\n",
      "Training Loss: 6.1061\n",
      "Training Loss: 0.2972\n",
      "Training Loss: 0.5353\n",
      "Training Loss: 1.4292\n",
      "Training Loss: 1.4027\n",
      "Training Loss: 0.5925\n",
      "Training Loss: 1.1878\n",
      "Training Loss: 0.1159\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1958\n",
      "Training Loss: 1.8305\n",
      "Training Loss: 0.6483\n",
      "Training Loss: 0.6233\n",
      "Training Loss: 3.5281\n",
      "Training Loss: 0.3026\n",
      "Training Loss: 0.5766\n",
      "Training Loss: 3.9024\n",
      "Training Loss: 2.9977\n",
      "Training Loss: 29.4369\n",
      "Training Loss: 27.3550\n",
      "Training Loss: 1.1928\n",
      "Training Loss: 0.4805\n",
      "Training Loss: 0.8935\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.6472\n",
      "Training Loss: 3.3037\n",
      "Training Loss: 0.1221\n",
      "Training Loss: 23.3801\n",
      "Training Loss: 0.1536\n",
      "Training Loss: 33.7375\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.4877\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 51.9225\n",
      "Training Loss: 0.7671\n",
      "Training Loss: 1.3820\n",
      "Training Loss: 0.1869\n",
      "Training Loss: 1.3227\n",
      "Training Loss: 0.8656\n",
      "Training Loss: 0.7176\n",
      "Training Loss: 1.3731\n",
      "Training Loss: 35.4794\n",
      "Training Loss: 18.9754\n",
      "Training Loss: 16.9869\n",
      "Training Loss: 21.4803\n",
      "Training Loss: 1.9587\n",
      "Training Loss: 4.9934\n",
      "Training Loss: 17.4901\n",
      "Training Loss: 0.1791\n",
      "Training Loss: 2.4261\n",
      "Training Loss: 0.2786\n",
      "Training Loss: 0.5390\n",
      "Training Loss: 0.4283\n",
      "Training Loss: 34.7952\n",
      "Training Loss: 45.2256\n",
      "Training Loss: 0.1819\n",
      "Training Loss: 1.5032\n",
      "Training Loss: 0.6970\n",
      "Training Loss: 0.1560\n",
      "Training Loss: 3.9122\n",
      "Training Loss: 2.2853\n",
      "Training Loss: 0.4824\n",
      "Training Loss: 48.2648\n",
      "Training Loss: 3.6032\n",
      "Training Loss: 1.7713\n",
      "Training Loss: 54.4554\n",
      "Training Loss: 2.1770\n",
      "Training Loss: 4.9981\n",
      "Training Loss: 52.4785\n",
      "Training Loss: 1.1462\n",
      "Training Loss: 0.9588\n",
      "Training Loss: 2.4484\n",
      "Training Loss: 10.4776\n",
      "Training Loss: 2.9118\n",
      "Training Loss: 23.4947\n",
      "Training Loss: 21.4948\n",
      "Training Loss: 0.9996\n",
      "Training Loss: 31.4922\n",
      "Training Loss: 7.9976\n",
      "Training Loss: 27.9956\n",
      "Training Loss: 6.9971\n",
      "Training Loss: 0.4807\n",
      "Training Loss: 12.9923\n",
      "Training Loss: 1.4128\n",
      "Training Loss: 12.9971\n",
      "Training Loss: 34.9732\n",
      "Training Loss: 0.4815\n",
      "Training Loss: 25.6770\n",
      "Training Loss: 46.8382\n",
      "Training Loss: 2.9606\n",
      "Training Loss: 1.9741\n",
      "Training Loss: 0.3866\n",
      "Training Loss: 34.4990\n",
      "Training Loss: 2.9906\n",
      "Training Loss: 1.2961\n",
      "Training Loss: 46.7796\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.9823\n",
      "Training Loss: 0.4782\n",
      "Training Loss: 0.9401\n",
      "Training Loss: 1.4103\n",
      "Training Loss: 0.4091\n",
      "Training Loss: 0.9996\n",
      "Training Loss: 2.3325\n",
      "Training Loss: 40.4341\n",
      "Training Loss: 1.8514\n",
      "Training Loss: 0.4631\n",
      "Training Loss: 0.4345\n",
      "Training Loss: 0.9156\n",
      "Training Loss: 0.4058\n",
      "Training Loss: 2.3875\n",
      "Training Loss: 17.9946\n",
      "Training Loss: 2.8531\n",
      "Training Loss: 1.4728\n",
      "Training Loss: 3.9984\n",
      "Training Loss: 1.3689\n",
      "Training Loss: 0.4551\n",
      "Training Loss: 46.9934\n",
      "Training Loss: 0.9788\n",
      "Training Loss: 4.3820\n",
      "Training Loss: 1.4728\n",
      "Training Loss: 33.1492\n",
      "Training Loss: 0.3491\n",
      "Training Loss: 1.4911\n",
      "Training Loss: 0.4428\n",
      "Training Loss: 0.6748\n",
      "Training Loss: 0.3012\n",
      "Training Loss: 61.4740\n",
      "Training Loss: 3.3674\n",
      "Training Loss: 3.9357\n",
      "Training Loss: 2.2221\n",
      "Training Loss: 55.9055\n",
      "Training Loss: 1.2870\n",
      "Training Loss: 0.3303\n",
      "Training Loss: 5.9498\n",
      "Training Loss: 54.4040\n",
      "Training Loss: 20.5138\n",
      "Training Loss: 2.3851\n",
      "Training Loss: 0.2457\n",
      "Training Loss: 44.8826\n",
      "Training Loss: 2.7123\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.3640\n",
      "Training Loss: 0.5703\n",
      "Training Loss: 3.7629\n",
      "Training Loss: 0.4020\n",
      "Training Loss: 0.5189\n",
      "Training Loss: 0.2856\n",
      "Training Loss: 1.4588\n",
      "Training Loss: 0.2163\n",
      "Training Loss: 1.0363\n",
      "Training Loss: 1.2728\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.3421\n",
      "Training Loss: 5.9528\n",
      "Training Loss: 13.4176\n",
      "Training Loss: 0.9797\n",
      "Training Loss: 1.6217\n",
      "Training Loss: 0.4440\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.4545\n",
      "Training Loss: 0.3548\n",
      "Training Loss: 0.5642\n",
      "Training Loss: 0.4205\n",
      "Training Loss: 46.4644\n",
      "Training Loss: 15.6260\n",
      "Training Loss: 1.7907\n",
      "Training Loss: 2.5795\n",
      "Training Loss: 34.9075\n",
      "Training Loss: 0.3298\n",
      "Training Loss: 0.2886\n",
      "Training Loss: 19.8719\n",
      "Training Loss: 0.2506\n",
      "Training Loss: 2.4128\n",
      "Training Loss: 0.3001\n",
      "Training Loss: 0.6323\n",
      "Training Loss: 0.6714\n",
      "Training Loss: 0.2778\n",
      "Training Loss: 0.4611\n",
      "Training Loss: 0.3067\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.5616\n",
      "Training Loss: 0.5132\n",
      "Training Loss: 1.3049\n",
      "Training Loss: 0.2430\n",
      "Training Loss: 0.9740\n",
      "Training Loss: 0.9111\n",
      "Training Loss: 1.3246\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.3952\n",
      "Training Loss: 0.6707\n",
      "Training Loss: 0.9842\n",
      "Training Loss: 0.8333\n",
      "Training Loss: 0.5919\n",
      "Training Loss: 0.6733\n",
      "Training Loss: 0.2857\n",
      "Training Loss: 4.1003\n",
      "Training Loss: 0.4704\n",
      "Training Loss: 0.8155\n",
      "Training Loss: 16.1439\n",
      "Training Loss: 0.2374\n",
      "Training Loss: 0.4570\n",
      "Training Loss: 0.1958\n",
      "Training Loss: 0.8301\n",
      "Training Loss: 0.1576\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.4405\n",
      "Training Loss: 0.1761\n",
      "Training Loss: 1.5448\n",
      "Training Loss: 0.2627\n",
      "Training Loss: 0.9609\n",
      "Training Loss: 0.7384\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.5274\n",
      "Training Loss: 19.4574\n",
      "Training Loss: 1.3405\n",
      "Training Loss: 0.3784\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1733\n",
      "Training Loss: 0.2309\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.2989\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2808\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2214\n",
      "Training Loss: 0.4074\n",
      "Training Loss: 2.3051\n",
      "Training Loss: 0.2953\n",
      "Training Loss: 22.9066\n",
      "Training Loss: 1.7925\n",
      "Training Loss: 0.3846\n",
      "Training Loss: 1.1217\n",
      "Training Loss: 0.4085\n",
      "Training Loss: 0.5142\n",
      "Training Loss: 0.5018\n",
      "Training Loss: 0.2225\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.3122\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.4729\n",
      "Training Loss: 0.4039\n",
      "Training Loss: 0.4796\n",
      "Training Loss: 0.8745\n",
      "Training Loss: 0.5410\n",
      "Training Loss: 3.6170\n",
      "Training Loss: 0.1778\n",
      "Training Loss: 0.1968\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1914\n",
      "Training Loss: 1.2512\n",
      "Training Loss: 5.8515\n",
      "Training Loss: 0.1811\n",
      "Training Loss: 0.3055\n",
      "Training Loss: 1.3239\n",
      "Training Loss: 0.7236\n",
      "Training Loss: 0.8988\n",
      "Training Loss: 0.3026\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 18.1223\n",
      "Training Loss: 0.1592\n",
      "Training Loss: 3.3834\n",
      "Training Loss: 1.5623\n",
      "Training Loss: 0.1465\n",
      "Training Loss: 0.6686\n",
      "Training Loss: 0.6465\n",
      "Training Loss: 15.7827\n",
      "Training Loss: 0.2816\n",
      "Training Loss: 0.2426\n",
      "Training Loss: 0.2013\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 5.2629\n",
      "Training Loss: 0.5625\n",
      "Training Loss: 0.3231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1325\n",
      "Training Loss: 2.1136\n",
      "Training Loss: 0.1672\n",
      "Training Loss: 1.1299\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 13.5727\n",
      "Training Loss: 0.3468\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.3660\n",
      "Training Loss: 1.8532\n",
      "Training Loss: 0.3972\n",
      "Training Loss: 1.2237\n",
      "Training Loss: 40.0543\n",
      "Training Loss: 0.1458\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1299\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.9771\n",
      "Training Loss: 0.1430\n",
      "Training Loss: 0.8146\n",
      "Training Loss: 0.2802\n",
      "Training Loss: 0.1614\n",
      "Training Loss: 0.1326\n",
      "Training Loss: 0.5529\n",
      "Training Loss: 0.4160\n",
      "Training Loss: 19.8719\n",
      "Training Loss: 1.0415\n",
      "Training Loss: 0.3269\n",
      "Training Loss: 1.7057\n",
      "Training Loss: 2.7250\n",
      "Training Loss: 0.3713\n",
      "Training Loss: 0.1251\n",
      "Training Loss: 2.1654\n",
      "Training Loss: 0.3557\n",
      "Training Loss: 0.8074\n",
      "Training Loss: 0.4043\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.5493\n",
      "Training Loss: 0.1326\n",
      "Training Loss: 0.2614\n",
      "Training Loss: 0.2268\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 9.0692\n",
      "Training Loss: 0.6481\n",
      "Training Loss: 0.2387\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1070\n",
      "Training Loss: 0.4207\n",
      "Training Loss: 0.2146\n",
      "Training Loss: 0.1044\n",
      "Training Loss: 0.4125\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1233\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.4708\n",
      "Training Loss: 0.7076\n",
      "Training Loss: 0.3941\n",
      "Training Loss: 0.9134\n",
      "Training Loss: 1.7393\n",
      "Training Loss: 0.4640\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.9934\n",
      "Training Loss: 1.2917\n",
      "Training Loss: 0.1590\n",
      "Training Loss: 1.1238\n",
      "Training Loss: 0.3073\n",
      "Training Loss: 0.3685\n",
      "Training Loss: 0.1305\n",
      "Training Loss: 0.0846\n",
      "Training Loss: 0.3602\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.8008\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1328\n",
      "Training Loss: 0.8487\n",
      "Training Loss: 1.7597\n",
      "Training Loss: 0.2393\n",
      "Training Loss: 0.3811\n",
      "Training Loss: 0.2354\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 2.5472\n",
      "Training Loss: 0.3295\n",
      "Training Loss: 60.3659\n",
      "Training Loss: 0.1306\n",
      "Training Loss: 0.2015\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.2600\n",
      "Training Loss: 2.1370\n",
      "Training Loss: 4.0594\n",
      "Training Loss: 0.9046\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.3487\n",
      "Training Loss: 0.3502\n",
      "Training Loss: 0.3621\n",
      "Training Loss: 0.4427\n",
      "Training Loss: 0.1028\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.0662\n",
      "Training Loss: 0.6480\n",
      "Training Loss: 0.5718\n",
      "Training Loss: 0.3559\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1884\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.4104\n",
      "Training Loss: 2.1054\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.4309\n",
      "Training Loss: 2.1970\n",
      "Training Loss: 1.3768\n",
      "Training Loss: 0.1695\n",
      "Training Loss: 0.3049\n",
      "Training Loss: 0.2244\n",
      "Training Loss: 2.0561\n",
      "Training Loss: 1.1417\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1179\n",
      "Training Loss: 0.2373\n",
      "Training Loss: 0.7475\n",
      "Training Loss: 13.5938\n",
      "Training Loss: 3.4807\n",
      "Training Loss: 0.6490\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.5043\n",
      "Training Loss: 2.8417\n",
      "Training Loss: 0.1673\n",
      "Training Loss: 0.2023\n",
      "Training Loss: 0.4625\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2689\n",
      "Training Loss: 0.1482\n",
      "Training Loss: 0.1992\n",
      "Training Loss: 0.2423\n",
      "Training Loss: 0.3637\n",
      "Training Loss: 0.3841\n",
      "Training Loss: 2.7649\n",
      "Training Loss: 1.4460\n",
      "Training Loss: 2.3860\n",
      "Training Loss: 1.2388\n",
      "Training Loss: 0.1449\n",
      "Training Loss: 0.2209\n",
      "Training Loss: 0.3278\n",
      "Training Loss: 0.2221\n",
      "Training Loss: 1.1119\n",
      "Training Loss: 0.1898\n",
      "Training Loss: 0.6024\n",
      "Training Loss: 0.2700\n",
      "Training Loss: 0.9999\n",
      "Training Loss: 0.1591\n",
      "Training Loss: 9.7683\n",
      "Training Loss: 20.8336\n",
      "Training Loss: 28.5637\n",
      "Training Loss: 0.9936\n",
      "Training Loss: 0.1257\n",
      "Training Loss: 1.4936\n",
      "Training Loss: 2.6210\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.8009\n",
      "Training Loss: 0.6715\n",
      "Training Loss: 0.2095\n",
      "Training Loss: 0.4208\n",
      "Training Loss: 1.8276\n",
      "Training Loss: 1.7424\n",
      "Training Loss: 48.7983\n",
      "Training Loss: 0.3155\n",
      "Training Loss: 0.1651\n",
      "Training Loss: 0.7962\n",
      "Training Loss: 1.4466\n",
      "Training Loss: 13.6995\n",
      "Training Loss: 1.2150\n",
      "Training Loss: 34.3990\n",
      "Training Loss: 2.4279\n",
      "Training Loss: 15.3833\n",
      "Training Loss: 61.4536\n",
      "Training Loss: 2.1869\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.7196\n",
      "Training Loss: 67.7700\n",
      "Training Loss: 2.0826\n",
      "Training Loss: 4.7141\n",
      "Training Loss: 2.3819\n",
      "Training Loss: 0.5512\n",
      "Training Loss: 2.9469\n",
      "Training Loss: 1.5465\n",
      "Training Loss: 2.8769\n",
      "Training Loss: 1.7185\n",
      "Training Loss: 2.3823\n",
      "Training Loss: 27.4885\n",
      "Training Loss: 1.2948\n",
      "Training Loss: 1.6099\n",
      "Training Loss: 3.4181\n",
      "Training Loss: 2.1067\n",
      "Training Loss: 1.9394\n",
      "Training Loss: 0.8807\n",
      "Training Loss: 0.8131\n",
      "Training Loss: 48.1375\n",
      "Training Loss: 0.1596\n",
      "Training Loss: 1.6720\n",
      "Training Loss: 0.4962\n",
      "Training Loss: 2.8733\n",
      "Training Loss: 0.7947\n",
      "Training Loss: 0.1836\n",
      "Training Loss: 0.4830\n",
      "Training Loss: 0.9181\n",
      "Training Loss: 22.2399\n",
      "Training Loss: 45.9633\n",
      "Training Loss: 0.1042\n",
      "Training Loss: 0.6956\n",
      "Training Loss: 0.4646\n",
      "Training Loss: 0.1555\n",
      "Training Loss: 0.1908\n",
      "Training Loss: 0.1309\n",
      "Training Loss: 0.2840\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.8845\n",
      "Training Loss: 0.9037\n",
      "Training Loss: 0.1469\n",
      "Training Loss: 70.2076\n",
      "Training Loss: 0.2378\n",
      "Training Loss: 56.0764\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.9810\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 41.5099\n",
      "Training Loss: 3.3336\n",
      "Training Loss: 1.2231\n",
      "Training Loss: 0.6590\n",
      "Training Loss: 0.5951\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 2.4492\n",
      "Training Loss: 0.1752\n",
      "Training Loss: 1.2490\n",
      "Training Loss: 0.3076\n",
      "Training Loss: 0.4387\n",
      "Training Loss: 0.5595\n",
      "Training Loss: 1.9745\n",
      "Training Loss: 0.1179\n",
      "Training Loss: 0.2911\n",
      "Training Loss: 0.1527\n",
      "Training Loss: 2.3621\n",
      "Training Loss: 0.2914\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.0706\n",
      "Training Loss: 0.9491\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.7260\n",
      "Training Loss: 0.4754\n",
      "Training Loss: 0.4486\n",
      "Training Loss: 1.1884\n",
      "Training Loss: 9.0704\n",
      "Training Loss: 1.2864\n",
      "Training Loss: 0.6799\n",
      "Training Loss: 0.3217\n",
      "Training Loss: 0.1580\n",
      "Training Loss: 1.0018\n",
      "Training Loss: 0.1707\n",
      "Training Loss: 0.3196\n",
      "Training Loss: 0.5645\n",
      "Training Loss: 0.3159\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.6037\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.4575\n",
      "Training Loss: 3.4057\n",
      "Training Loss: 0.7532\n",
      "Training Loss: 0.3163\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.4995\n",
      "Training Loss: 20.4608\n",
      "Training Loss: 0.9210\n",
      "Training Loss: 1.3069\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.9718\n",
      "Training Loss: 3.9857\n",
      "Training Loss: 0.5755\n",
      "Training Loss: 0.1603\n",
      "Training Loss: 0.2942\n",
      "Training Loss: 0.1540\n",
      "Training Loss: 1.1138\n",
      "Training Loss: 0.2689\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1351\n",
      "Training Loss: 0.6510\n",
      "Training Loss: 0.5879\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.6387\n",
      "Training Loss: 0.2306\n",
      "Training Loss: 0.1173\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.5456\n",
      "Training Loss: 5.3115\n",
      "Training Loss: 1.6006\n",
      "Training Loss: 0.7402\n",
      "Training Loss: 0.2033\n",
      "Training Loss: 0.5966\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.6329\n",
      "Training Loss: 0.1125\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1938\n",
      "Training Loss: 0.4764\n",
      "Training Loss: 0.4512\n",
      "Training Loss: 0.2392\n",
      "Training Loss: 1.4221\n",
      "Training Loss: 0.3820\n",
      "Training Loss: 0.3737\n",
      "Training Loss: 4.9259\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1760\n",
      "Training Loss: 0.2863\n",
      "Training Loss: 0.5060\n",
      "Training Loss: 1.7127\n",
      "Training Loss: 0.1640\n",
      "Training Loss: 1.6199\n",
      "Training Loss: 0.2207\n",
      "Training Loss: 0.8665\n",
      "Training Loss: 0.3703\n",
      "Training Loss: 4.8045\n",
      "Training Loss: 0.7351\n",
      "Training Loss: 0.4513\n",
      "Training Loss: 0.7728\n",
      "Training Loss: 0.5137\n",
      "Training Loss: 0.4490\n",
      "Training Loss: 0.9554\n",
      "Training Loss: 0.8109\n",
      "Training Loss: 1.5193\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.7561\n",
      "Training Loss: 0.0821\n",
      "Training Loss: 0.6341\n",
      "Training Loss: 0.1409\n",
      "Training Loss: 1.7838\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.8569\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.6247\n",
      "Training Loss: 0.3359\n",
      "Training Loss: 0.2179\n",
      "Training Loss: 0.3001\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.4162\n",
      "Training Loss: 0.5263\n",
      "Training Loss: 0.3972\n",
      "Training Loss: 0.3774\n",
      "Training Loss: 0.5000\n",
      "Training Loss: 0.1712\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0802\n",
      "Training Loss: 1.3137\n",
      "Training Loss: 3.2416\n",
      "Training Loss: 0.3517\n",
      "Training Loss: 0.3171\n",
      "Training Loss: 0.4449\n",
      "Training Loss: 0.4328\n",
      "Training Loss: 0.3231\n",
      "Training Loss: 1.3187\n",
      "Training Loss: 1.3202\n",
      "Training Loss: 3.0629\n",
      "Training Loss: 0.6078\n",
      "Training Loss: 16.7642\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2472\n",
      "Training Loss: 0.5637\n",
      "Training Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 13.6269\n",
      "Training Loss: 0.8635\n",
      "Training Loss: 0.2653\n",
      "Training Loss: 0.5041\n",
      "Training Loss: 0.2995\n",
      "Training Loss: 2.0692\n",
      "Training Loss: 0.8112\n",
      "Training Loss: 0.1744\n",
      "Training Loss: 0.7926\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 31.7629\n",
      "Training Loss: 1.6736\n",
      "Training Loss: 2.6799\n",
      "Training Loss: 50.5002\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 2.0560\n",
      "Training Loss: 0.3548\n",
      "Training Loss: 0.4510\n",
      "Training Loss: 0.4515\n",
      "Training Loss: 0.8970\n",
      "Training Loss: 0.3172\n",
      "Training Loss: 0.7294\n",
      "Training Loss: 7.9026\n",
      "Training Loss: 0.6998\n",
      "Training Loss: 0.2722\n",
      "Training Loss: 0.1706\n",
      "Training Loss: 0.9463\n",
      "Training Loss: 0.1510\n",
      "Training Loss: 0.1296\n",
      "Training Loss: 0.1458\n",
      "Training Loss: 0.5689\n",
      "Training Loss: 16.9912\n",
      "Training Loss: 0.8632\n",
      "Training Loss: 2.2971\n",
      "Training Loss: 0.2904\n",
      "Training Loss: 29.5633\n",
      "Training Loss: 0.2663\n",
      "Training Loss: 0.4182\n",
      "Training Loss: 0.4757\n",
      "Training Loss: 0.2991\n",
      "Training Loss: 0.1287\n",
      "Training Loss: 0.5000\n",
      "Training Loss: 0.2788\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.5489\n",
      "Training Loss: 0.2518\n",
      "Training Loss: 14.8404\n",
      "Training Loss: 0.0961\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.4744\n",
      "Training Loss: 0.2834\n",
      "Training Loss: 0.1115\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.8303\n",
      "Training Loss: 0.6595\n",
      "Training Loss: 0.4141\n",
      "Training Loss: 0.6008\n",
      "Training Loss: 1.0462\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 2.9063\n",
      "Training Loss: 0.3866\n",
      "Training Loss: 0.8135\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.8183\n",
      "Training Loss: 0.1353\n",
      "Training Loss: 0.9636\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.2586\n",
      "Training Loss: 0.4854\n",
      "Training Loss: 4.5129\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.9430\n",
      "Training Loss: 0.4868\n",
      "Training Loss: 0.2105\n",
      "Training Loss: 0.6330\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.3978\n",
      "Training Loss: 0.1825\n",
      "Training Loss: 1.2220\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0827\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.6445\n",
      "Training Loss: 0.5519\n",
      "Training Loss: 0.1066\n",
      "Training Loss: 0.0890\n",
      "Training Loss: 1.2679\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.9241\n",
      "Training Loss: 1.4657\n",
      "Training Loss: 0.5187\n",
      "Training Loss: 0.5546\n",
      "Training Loss: 1.5358\n",
      "Training Loss: 0.1038\n",
      "Training Loss: 0.1576\n",
      "Training Loss: 1.1120\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.7515\n",
      "Training Loss: 0.9068\n",
      "Training Loss: 0.0897\n",
      "Training Loss: 0.2053\n",
      "Training Loss: 0.7612\n",
      "Training Loss: 2.3950\n",
      "Training Loss: 0.2120\n",
      "Training Loss: 0.1884\n",
      "Training Loss: 0.5442\n",
      "Training Loss: 1.3528\n",
      "Training Loss: 0.1074\n",
      "Training Loss: 0.3560\n",
      "Training Loss: 0.8388\n",
      "Training Loss: 0.3608\n",
      "Training Loss: 0.0979\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2607\n",
      "Training Loss: 0.8925\n",
      "Training Loss: 0.6328\n",
      "Training Loss: 0.2863\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2913\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.3046\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0993\n",
      "Training Loss: 0.2350\n",
      "Training Loss: 0.3058\n",
      "Training Loss: 0.3208\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2753\n",
      "Training Loss: 0.6857\n",
      "Training Loss: 1.7523\n",
      "Training Loss: 2.7627\n",
      "Training Loss: 0.0987\n",
      "Training Loss: 0.5196\n",
      "Training Loss: 0.9206\n",
      "Training Loss: 0.4107\n",
      "Training Loss: 2.3112\n",
      "Training Loss: 0.5242\n",
      "Training Loss: 0.1476\n",
      "Training Loss: 0.3354\n",
      "Training Loss: 0.4475\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2944\n",
      "Training Loss: 1.5708\n",
      "Training Loss: 1.8870\n",
      "Training Loss: 0.7011\n",
      "Training Loss: 0.7483\n",
      "Training Loss: 22.4674\n",
      "Training Loss: 0.6582\n",
      "Training Loss: 0.2804\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 2.6385\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.7710\n",
      "Training Loss: 1.7682\n",
      "Training Loss: 0.4566\n",
      "Training Loss: 1.3891\n",
      "Training Loss: 0.2146\n",
      "Training Loss: 0.4455\n",
      "Training Loss: 0.3055\n",
      "Training Loss: 0.2847\n",
      "Training Loss: 0.7957\n",
      "Training Loss: 0.6833\n",
      "Training Loss: 0.2756\n",
      "Training Loss: 0.3643\n",
      "Training Loss: 0.1053\n",
      "Training Loss: 0.3461\n",
      "Training Loss: 0.1351\n",
      "Training Loss: 1.8475\n",
      "Training Loss: 0.5117\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1309\n",
      "Training Loss: 0.2242\n",
      "Training Loss: 0.4952\n",
      "Training Loss: 0.1130\n",
      "Training Loss: 0.8872\n",
      "Training Loss: 0.6748\n",
      "Training Loss: 0.2675\n",
      "Training Loss: 2.1409\n",
      "Training Loss: 0.5014\n",
      "Training Loss: 0.2448\n",
      "Training Loss: 0.8335\n",
      "Training Loss: 0.0968\n",
      "Training Loss: 0.1711\n",
      "Training Loss: 0.3268\n",
      "Training Loss: 0.4767\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.5777\n",
      "Training Loss: 0.4249\n",
      "Training Loss: 0.8472\n",
      "Training Loss: 1.0252\n",
      "Training Loss: 0.8557\n",
      "Training Loss: 0.2606\n",
      "Training Loss: 2.4273\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.4313\n",
      "Training Loss: 0.6082\n",
      "Training Loss: 1.4671\n",
      "Training Loss: 0.1246\n",
      "Training Loss: 1.5674\n",
      "Training Loss: 0.1570\n",
      "Training Loss: 1.9823\n",
      "Training Loss: 0.1284\n",
      "Training Loss: 0.9273\n",
      "Training Loss: 0.6995\n",
      "Training Loss: 0.0731\n",
      "Training Loss: 0.1304\n",
      "Training Loss: 0.1058\n",
      "Training Loss: 0.3231\n",
      "Training Loss: 0.6575\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.7401\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.6634\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.8452\n",
      "Training Loss: 0.3808\n",
      "Training Loss: 0.4286\n",
      "Training Loss: 0.1851\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.5213\n",
      "Training Loss: 0.6331\n",
      "Training Loss: 2.3922\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1432\n",
      "Training Loss: 2.5304\n",
      "Training Loss: 0.2558\n",
      "Training Loss: 0.1473\n",
      "Training Loss: 0.2692\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1362\n",
      "Training Loss: 1.5131\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1341\n",
      "Training Loss: 0.1017\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.9106\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.3871\n",
      "Training Loss: 0.3390\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.7986\n",
      "Training Loss: 1.6395\n",
      "Training Loss: 0.6903\n",
      "Training Loss: 0.1344\n",
      "Training Loss: 0.1216\n",
      "Training Loss: 2.1688\n",
      "Training Loss: 0.6236\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2768\n",
      "Training Loss: 0.4540\n",
      "Training Loss: 0.2393\n",
      "Training Loss: 2.1878\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.3180\n",
      "Training Loss: 1.2151\n",
      "Training Loss: 2.9501\n",
      "Training Loss: 0.2874\n",
      "Training Loss: 0.3020\n",
      "Training Loss: 0.1090\n",
      "Training Loss: 1.1685\n",
      "Training Loss: 0.2530\n",
      "Training Loss: 0.2850\n",
      "Training Loss: 0.8153\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2774\n",
      "Training Loss: 1.2449\n",
      "Training Loss: 0.4159\n",
      "Training Loss: 1.6145\n",
      "Training Loss: 0.1990\n",
      "Training Loss: 3.4250\n",
      "Training Loss: 1.2023\n",
      "Training Loss: 0.1799\n",
      "Training Loss: 0.7283\n",
      "Training Loss: 0.5481\n",
      "Training Loss: 1.9245\n",
      "Training Loss: 1.0539\n",
      "Training Loss: 0.2629\n",
      "Training Loss: 0.6039\n",
      "Training Loss: 0.4710\n",
      "Training Loss: 0.1746\n",
      "Training Loss: 0.9993\n",
      "Training Loss: 0.2311\n",
      "Training Loss: 0.3232\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2594\n",
      "Training Loss: 1.4098\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2841\n",
      "Training Loss: 0.2088\n",
      "Training Loss: 0.4088\n",
      "Training Loss: 0.3483\n",
      "Training Loss: 0.1337\n",
      "Training Loss: 0.6049\n",
      "Training Loss: 0.1804\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2597\n",
      "Training Loss: 1.0627\n",
      "Training Loss: 0.0954\n",
      "Training Loss: 0.1127\n",
      "Training Loss: 0.9714\n",
      "Training Loss: 2.1958\n",
      "Training Loss: 0.5315\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.9283\n",
      "Training Loss: 2.1298\n",
      "Training Loss: 1.5561\n",
      "Training Loss: 2.6256\n",
      "Training Loss: 0.6610\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.3362\n",
      "Training Loss: 0.2333\n",
      "Training Loss: 0.1105\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.5956\n",
      "Training Loss: 0.0857\n",
      "Training Loss: 0.1549\n",
      "Training Loss: 0.3648\n",
      "Training Loss: 0.4978\n",
      "Training Loss: 2.4471\n",
      "Training Loss: 0.2637\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.4161\n",
      "Training Loss: 0.2708\n",
      "Training Loss: 1.6951\n",
      "Training Loss: 0.1389\n",
      "Training Loss: 0.8251\n",
      "Training Loss: 0.2947\n",
      "Training Loss: 0.0401\n",
      "Training Loss: 0.0843\n",
      "Training Loss: 0.4604\n",
      "Training Loss: 1.8837\n",
      "Training Loss: 0.2301\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.8858\n",
      "Training Loss: 0.5184\n",
      "Training Loss: 0.1169\n",
      "Training Loss: 1.0342\n",
      "Training Loss: 1.3725\n",
      "Training Loss: 0.1782\n",
      "Training Loss: 0.1188\n",
      "Training Loss: 0.1388\n",
      "Training Loss: 0.1720\n",
      "Training Loss: 1.3564\n",
      "Training Loss: 1.5726\n",
      "Training Loss: 0.3289\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 2.2142\n",
      "Training Loss: 1.5306\n",
      "Training Loss: 0.7924\n",
      "Training Loss: 0.6711\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.3008\n",
      "Training Loss: 0.1579\n",
      "Training Loss: 0.2646\n",
      "Training Loss: 0.1452\n",
      "Training Loss: 0.1418\n",
      "Training Loss: 0.2653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5052\n",
      "Training Loss: 0.0870\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.4778\n",
      "Training Loss: 0.1692\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0926\n",
      "Training Loss: 0.2550\n",
      "Training Loss: 1.9159\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0605\n",
      "Training Loss: 0.5612\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0757\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.3734\n",
      "Training Loss: 0.1878\n",
      "Training Loss: 0.6394\n",
      "Training Loss: 1.5185\n",
      "Training Loss: 0.1057\n",
      "Training Loss: 0.7707\n",
      "Training Loss: 0.7693\n",
      "Training Loss: 1.6871\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.4918\n",
      "Training Loss: 0.0936\n",
      "Training Loss: 0.4545\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0708\n",
      "Training Loss: 0.7673\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1250\n",
      "Training Loss: 0.0765\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.5365\n",
      "Training Loss: 0.2195\n",
      "Training Loss: 0.1453\n",
      "Training Loss: 0.1076\n",
      "Training Loss: 0.6617\n",
      "Training Loss: 0.6130\n",
      "Training Loss: 0.1168\n",
      "Training Loss: 0.1592\n",
      "Training Loss: 0.1167\n",
      "Training Loss: 0.6374\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0630\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.9374\n",
      "Training Loss: 0.1460\n",
      "Training Loss: 0.1532\n",
      "Training Loss: 0.1838\n",
      "Training Loss: 0.6488\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0897\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0788\n",
      "Training Loss: 0.2893\n",
      "Training Loss: 0.6798\n",
      "Training Loss: 0.6388\n",
      "Training Loss: 0.3550\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 2.6965\n",
      "Training Loss: 0.1712\n",
      "Training Loss: 0.2641\n",
      "Training Loss: 1.1996\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 39.5164\n",
      "Training Loss: 0.1882\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.5651\n",
      "Training Loss: 0.4416\n",
      "Training Loss: 3.2796\n",
      "Training Loss: 0.3066\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2726\n",
      "Training Loss: 1.1975\n",
      "Training Loss: 0.7727\n",
      "Training Loss: 0.1986\n",
      "Training Loss: 1.1580\n",
      "Training Loss: 1.8206\n",
      "Training Loss: 0.1840\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 2.7280\n",
      "Training Loss: 0.6549\n",
      "Training Loss: 1.7139\n",
      "Training Loss: 0.4864\n",
      "Training Loss: 0.1137\n",
      "Training Loss: 0.6455\n",
      "Training Loss: 0.5629\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.7023\n",
      "Training Loss: 1.6109\n",
      "Training Loss: 0.9370\n",
      "Training Loss: 0.6066\n",
      "Training Loss: 0.2733\n",
      "Training Loss: 2.5055\n",
      "Training Loss: 23.3360\n",
      "Training Loss: 0.3323\n",
      "Training Loss: 3.1558\n",
      "Training Loss: 0.1681\n",
      "Training Loss: 0.1594\n",
      "Training Loss: 0.2582\n",
      "Training Loss: 0.5804\n",
      "Training Loss: 1.2754\n",
      "Training Loss: 0.5299\n",
      "Training Loss: 0.2050\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.0396\n",
      "Training Loss: 1.5320\n",
      "Training Loss: 0.4395\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.6863\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1734\n",
      "Training Loss: 0.2018\n",
      "Training Loss: 0.9671\n",
      "Training Loss: 0.1885\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2251\n",
      "Training Loss: 0.5283\n",
      "Training Loss: 0.3603\n",
      "Training Loss: 1.8231\n",
      "Training Loss: 0.1062\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.5350\n",
      "Training Loss: 0.6721\n",
      "Training Loss: 0.6349\n",
      "Training Loss: 0.9484\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1282\n",
      "Training Loss: 0.3409\n",
      "Training Loss: 0.4926\n",
      "Training Loss: 0.2100\n",
      "Training Loss: 0.1435\n",
      "Training Loss: 0.4226\n",
      "Training Loss: 0.6911\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2071\n",
      "Training Loss: 0.3800\n",
      "Training Loss: 0.6554\n",
      "Training Loss: 0.2992\n",
      "Training Loss: 0.0849\n",
      "Training Loss: 0.2999\n",
      "Training Loss: 0.3464\n",
      "Training Loss: 0.4204\n",
      "Training Loss: 0.0989\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.4259\n",
      "Training Loss: 1.0396\n",
      "Training Loss: 2.1981\n",
      "Training Loss: 0.5966\n",
      "Training Loss: 0.0594\n",
      "Training Loss: 0.1069\n",
      "Training Loss: 0.5487\n",
      "Training Loss: 1.9263\n",
      "Training Loss: 0.6822\n",
      "Training Loss: 2.9686\n",
      "Training Loss: 0.8074\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2319\n",
      "Training Loss: 0.2948\n",
      "Training Loss: 1.8474\n",
      "Training Loss: 0.1248\n",
      "Training Loss: 0.6905\n",
      "Training Loss: 0.4317\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2957\n",
      "Training Loss: 0.2558\n",
      "Training Loss: 0.2813\n",
      "Training Loss: 2.4493\n",
      "Training Loss: 0.4851\n",
      "Training Loss: 0.0548\n",
      "Training Loss: 0.8212\n",
      "Training Loss: 1.7638\n",
      "Training Loss: 0.1270\n",
      "Training Loss: 0.5397\n",
      "Training Loss: 0.0891\n",
      "Training Loss: 0.3270\n",
      "Training Loss: 0.3072\n",
      "Training Loss: 1.2564\n",
      "Training Loss: 0.4075\n",
      "Training Loss: 0.1034\n",
      "Training Loss: 0.0434\n",
      "Training Loss: 0.4559\n",
      "Training Loss: 0.2951\n",
      "Training Loss: 1.0852\n",
      "Training Loss: 1.8303\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.5686\n",
      "Training Loss: 1.1594\n",
      "Training Loss: 0.2135\n",
      "Training Loss: 0.0837\n",
      "Training Loss: 0.1924\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1837\n",
      "Training Loss: 0.4913\n",
      "Training Loss: 0.7768\n",
      "Training Loss: 0.5069\n",
      "Training Loss: 0.8266\n",
      "Training Loss: 0.3130\n",
      "Training Loss: 1.3869\n",
      "Training Loss: 0.1134\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 9.9590\n",
      "Training Loss: 0.1803\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.6649\n",
      "Training Loss: 0.3226\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1244\n",
      "Training Loss: 2.7436\n",
      "Training Loss: 0.4769\n",
      "Training Loss: 0.4327\n",
      "Training Loss: 0.3524\n",
      "Training Loss: 0.3103\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2859\n",
      "Training Loss: 0.2185\n",
      "Training Loss: 1.4215\n",
      "Training Loss: 0.4380\n",
      "Training Loss: 1.2816\n",
      "Training Loss: 0.8019\n",
      "Training Loss: 0.4877\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.6673\n",
      "Training Loss: 0.2899\n",
      "Training Loss: 0.2887\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.0454\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0587\n",
      "Training Loss: 0.0608\n",
      "Training Loss: 0.6030\n",
      "Training Loss: 0.8525\n",
      "Training Loss: 0.3977\n",
      "Training Loss: 1.1575\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1616\n",
      "Training Loss: 0.1829\n",
      "Training Loss: 0.5213\n",
      "Training Loss: 0.9373\n",
      "Training Loss: 0.4421\n",
      "Training Loss: 1.5219\n",
      "Training Loss: 2.7087\n",
      "Training Loss: 1.3683\n",
      "Training Loss: 0.2778\n",
      "Training Loss: 0.5989\n",
      "Training Loss: 0.3895\n",
      "Training Loss: 0.3472\n",
      "Training Loss: 0.3014\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2305\n",
      "Training Loss: 0.4618\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.3851\n",
      "Training Loss: 2.9508\n",
      "Training Loss: 1.5493\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1090\n",
      "Training Loss: 0.1124\n",
      "Training Loss: 0.1040\n",
      "Training Loss: 0.1972\n",
      "Training Loss: 1.1861\n",
      "Training Loss: 0.2711\n",
      "Training Loss: 0.1757\n",
      "Training Loss: 0.1202\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.3130\n",
      "Training Loss: 0.0963\n",
      "Training Loss: 0.6472\n",
      "Training Loss: 1.0124\n",
      "Training Loss: 0.2543\n",
      "Training Loss: 0.2462\n",
      "Training Loss: 12.4396\n",
      "Training Loss: 0.0821\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.0594\n",
      "Training Loss: 1.7483\n",
      "Training Loss: 2.3449\n",
      "Training Loss: 0.3139\n",
      "Training Loss: 1.0142\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.5257\n",
      "Training Loss: 0.9180\n",
      "Training Loss: 0.9828\n",
      "Training Loss: 0.2787\n",
      "Training Loss: 0.1523\n",
      "Training Loss: 0.3319\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2162\n",
      "Training Loss: 0.2829\n",
      "Training Loss: 0.6674\n",
      "Training Loss: 0.3355\n",
      "Training Loss: 1.0148\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1163\n",
      "Training Loss: 0.2201\n",
      "Training Loss: 0.0931\n",
      "Training Loss: 0.6232\n",
      "Training Loss: 0.4120\n",
      "Training Loss: 1.1759\n",
      "Training Loss: 0.1281\n",
      "Training Loss: 1.0379\n",
      "Training Loss: 0.3956\n",
      "Training Loss: 0.2442\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.7154\n",
      "Training Loss: 2.2611\n",
      "Training Loss: 0.1108\n",
      "Training Loss: 2.0793\n",
      "Training Loss: 1.7337\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.4695\n",
      "Training Loss: 0.7301\n",
      "Training Loss: 0.3342\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.0801\n",
      "Training Loss: 17.2073\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.5767\n",
      "Training Loss: 1.0004\n",
      "Training Loss: 0.5754\n",
      "Training Loss: 0.2307\n",
      "Training Loss: 1.6385\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.3792\n",
      "Training Loss: 0.1577\n",
      "Training Loss: 1.1979\n",
      "Training Loss: 0.8777\n",
      "Training Loss: 0.2941\n",
      "Training Loss: 0.8460\n",
      "Training Loss: 0.4142\n",
      "Training Loss: 0.6292\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.5484\n",
      "Training Loss: 0.1017\n",
      "Training Loss: 0.3710\n",
      "Training Loss: 1.5274\n",
      "Training Loss: 0.1207\n",
      "Training Loss: 1.3752\n",
      "Training Loss: 0.2199\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.6271\n",
      "Training Loss: 0.4005\n",
      "Training Loss: 0.3768\n",
      "Training Loss: 1.1786\n",
      "Training Loss: 0.9838\n",
      "Training Loss: 0.3285\n",
      "Training Loss: 0.6891\n",
      "Training Loss: 0.8568\n",
      "Training Loss: 2.4178\n",
      "Training Loss: 0.0978\n",
      "Training Loss: 0.2243\n",
      "Training Loss: 0.8229\n",
      "Training Loss: 0.1190\n",
      "Training Loss: 0.5003\n",
      "Training Loss: 0.1115\n",
      "Training Loss: 0.9480\n",
      "Training Loss: 0.1316\n",
      "Training Loss: 0.0505\n",
      "Training Loss: 0.6700\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2163\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.5023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0848\n",
      "Training Loss: 0.2310\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.7212\n",
      "Training Loss: 0.1547\n",
      "Training Loss: 0.1190\n",
      "Training Loss: 0.8868\n",
      "Training Loss: 3.1440\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.7634\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.6951\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1671\n",
      "Training Loss: 0.1407\n",
      "Training Loss: 0.3511\n",
      "Training Loss: 0.3038\n",
      "Training Loss: 0.1174\n",
      "Training Loss: 0.1425\n",
      "Training Loss: 2.4807\n",
      "Training Loss: 0.1975\n",
      "Training Loss: 0.2172\n",
      "Training Loss: 2.2836\n",
      "Training Loss: 1.7756\n",
      "Training Loss: 0.8803\n",
      "Training Loss: 0.5193\n",
      "Training Loss: 0.3580\n",
      "Training Loss: 2.4316\n",
      "Training Loss: 0.4750\n",
      "Training Loss: 0.1068\n",
      "Training Loss: 0.1370\n",
      "Training Loss: 0.1323\n",
      "Training Loss: 0.3576\n",
      "Training Loss: 0.3205\n",
      "Training Loss: 0.3553\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1241\n",
      "Training Loss: 0.5547\n",
      "Training Loss: 1.2283\n",
      "Training Loss: 0.1183\n",
      "Training Loss: 0.1008\n",
      "Training Loss: 0.2527\n",
      "Training Loss: 0.2604\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.5995\n",
      "Training Loss: 0.1610\n",
      "Training Loss: 0.7452\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.5064\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.4229\n",
      "Training Loss: 0.0743\n",
      "Training Loss: 0.3364\n",
      "Training Loss: 0.2343\n",
      "Training Loss: 0.4145\n",
      "Training Loss: 0.0769\n",
      "Training Loss: 0.2128\n",
      "Training Loss: 1.3862\n",
      "Training Loss: 0.3379\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0789\n",
      "Training Loss: 0.2834\n",
      "Training Loss: 0.3254\n",
      "Training Loss: 0.4706\n",
      "Training Loss: 0.6327\n",
      "Training Loss: 0.1723\n",
      "Training Loss: 0.0829\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1956\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.2683\n",
      "Training Loss: 0.2865\n",
      "Training Loss: 0.0750\n",
      "Training Loss: 0.1011\n",
      "Training Loss: 2.9800\n",
      "Training Loss: 0.5956\n",
      "Training Loss: 1.9960\n",
      "Training Loss: 0.9347\n",
      "Training Loss: 1.3186\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.6274\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.8684\n",
      "Training Loss: 1.1672\n",
      "Training Loss: 0.9938\n",
      "Training Loss: 0.1747\n",
      "Training Loss: 0.1307\n",
      "Training Loss: 0.3395\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.5480\n",
      "Training Loss: 0.7622\n",
      "Training Loss: 1.0870\n",
      "Training Loss: 0.1476\n",
      "Training Loss: 0.2230\n",
      "Training Loss: 0.3766\n",
      "Training Loss: 0.1357\n",
      "Training Loss: 0.2387\n",
      "Training Loss: 0.2270\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.4164\n",
      "Training Loss: 1.3364\n",
      "Training Loss: 1.6598\n",
      "Training Loss: 0.0930\n",
      "Training Loss: 0.2970\n",
      "Training Loss: 0.3901\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.9834\n",
      "Training Loss: 0.4067\n",
      "Training Loss: 1.4760\n",
      "Training Loss: 0.2524\n",
      "Training Loss: 0.3962\n",
      "Training Loss: 6.3148\n",
      "Training Loss: 1.6781\n",
      "Training Loss: 0.1754\n",
      "Training Loss: 1.9378\n",
      "Training Loss: 0.5343\n",
      "Training Loss: 0.8945\n",
      "Training Loss: 0.1593\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.2791\n",
      "Training Loss: 3.2483\n",
      "Training Loss: 0.7692\n",
      "Training Loss: 0.3274\n",
      "Training Loss: 3.1679\n",
      "Training Loss: 0.4486\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 3.3127\n",
      "Training Loss: 2.2938\n",
      "Training Loss: 1.3683\n",
      "Training Loss: 0.2713\n",
      "Training Loss: 1.1122\n",
      "Training Loss: 0.3580\n",
      "Training Loss: 1.0186\n",
      "Training Loss: 0.3297\n",
      "Training Loss: 0.4735\n",
      "Training Loss: 3.5122\n",
      "Training Loss: 0.1084\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1481\n",
      "Training Loss: 0.2074\n",
      "Training Loss: 0.4486\n",
      "Training Loss: 1.7442\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1930\n",
      "Training Loss: 0.1952\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.4862\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 0.1237\n",
      "Training Loss: 0.6261\n",
      "Training Loss: 0.1092\n",
      "Training Loss: 0.9978\n",
      "Training Loss: 0.3740\n",
      "Training Loss: 0.8904\n",
      "Training Loss: 1.0062\n",
      "Training Loss: 0.0000\n",
      "Training Loss: 1.8109\n",
      "Training Loss: 1.2936\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f4e3d0cd5299>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training complete in {:.0f}m {:.0f}s'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_elapsed\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_elapsed\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Finished training'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-f4e3d0cd5299>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m#             print(train_inputs.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;31m#             print(output.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m#             print(np.sum(output.cpu().detach().numpy()))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-0d8944fbf70a>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer6\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer7\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer8\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer9\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer10\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[1;31m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# use cumulative moving average\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                     \u001b[0mexponential_average_factor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = nn.L1Loss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 1\n",
    "epochs_no_improve_limit = 7\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "def train():\n",
    "    epochs_no_improve = 0\n",
    "    min_val_loss = np.Inf\n",
    "    since = time.time()\n",
    "    iteration = 0\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, epochs))\n",
    "        print('-' * 10)\n",
    "        val_loss = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        for train_inputs, train_labels in Generator(training_file, batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            train_inputs = train_inputs.transpose(0,3,1,2)\n",
    "#             print(train_inputs.shape)\n",
    "            \n",
    "            output = net(torch.from_numpy(train_inputs).to(device))\n",
    "#             print(output.shape)\n",
    "#             print(np.sum(output.cpu().detach().numpy()))\n",
    "            \n",
    "            train_labels = np.divide(train_labels, np.sum(train_labels))\n",
    "#             print(np.sum(train_labels))\n",
    "\n",
    "            \n",
    "            loss = loss_fn(output.cpu(), torch.from_numpy(train_labels).cpu())*(distance(output.cpu().detach().numpy(), train_labels)/4)\n",
    "            train_loss += loss\n",
    "            \n",
    "#             FileName = f'Data/Output/output-%04d.png' % iteration\n",
    "            \n",
    "#             cv2.imwrite(FileName, np.float32(output[0].cpu().detach().numpy()) )\n",
    "            \n",
    "#             fig = plt.figure(figsize = (32,24))\n",
    "#             ax1 = fig.add_subplot(2, 2, 1)\n",
    "#             ax1.set_title('Input red')\n",
    "#             ax2 = fig.add_subplot(2, 2, 2)\n",
    "#             ax2.set_title('Input green')\n",
    "#             ax3 = fig.add_subplot(2, 2, 3)\n",
    "#             ax3.set_title('Label')\n",
    "#             ax4 = fig.add_subplot(2, 2, 4)\n",
    "#             ax4.set_title('Output of the CNN')\n",
    "            \n",
    "#             sns.heatmap(train_inputs[0][2], vmin=0, vmax=1, cmap='gray', ax=ax1, cbar_kws={'shrink': .9})\n",
    "#             sns.heatmap(train_inputs[0][2], vmin=0, vmax=1, ax=ax2, cbar_kws={'shrink': .9})\n",
    "#             sns.heatmap((train_labels[0]), ax=ax3, cbar_kws={'shrink': .9})\n",
    "#             sns.heatmap((output[0].cpu().detach().numpy()), ax=ax4, cbar_kws={'shrink': .9})\n",
    "#             plt.show()\n",
    "\n",
    "\n",
    "#             writer.add_scalar('Loss/train', loss, iteration)\n",
    "\n",
    "            print('Training Loss: {:.4f}'.format(loss.item()))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            iteration += 1\n",
    "        \n",
    "        train_loss = train_loss / len(dataloaders['train'])\n",
    "        train_losses.append(float(train_loss))\n",
    "        \n",
    "        del train_inputs\n",
    "        del train_labels\n",
    "        del output\n",
    "        del loss\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for val_inputs, val_labels in dataloaders['val']:\n",
    "#                 torch.cuda.empty_cache()\n",
    "#                 output = net(val_inputs.to(device)).reshape(batch_size, output_height, output_width)\n",
    "#                 loss = loss_fn(output.cpu(), val_labels.cpu().reshape(batch_size, output_height, output_width))\n",
    "#                 val_loss += loss\n",
    "\n",
    "#             val_loss = val_loss / len(dataloaders['val'])\n",
    "#             val_losses.append(float(val_loss))\n",
    "#             print('-' * 10)\n",
    "#             print('Validation Loss: {:.4f}'.format(val_loss))\n",
    "\n",
    "#             if val_loss < min_val_loss:\n",
    "#                 torch.save({'state_dict': net.state_dict()}, 'Nets/pt-labi_CNN.pt')\n",
    "#                 epochs_no_improve = 0\n",
    "#                 min_val_loss = val_loss\n",
    "#             else:\n",
    "#                 epochs_no_improve += 1\n",
    "#                 if epochs_no_improve == epochs_no_improve_limit:\n",
    "#                     print('Early stopping initiated')\n",
    "#                     model = torch.load('Nets/pt-labi_CNN.pt')\n",
    "#                     print('Best model so far has been loaded')\n",
    "    print('Least validation Loss: {:4f}'.format(min_val_loss))\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Finished training')\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss:  41.30497\n",
      "Testing completed\n"
     ]
    }
   ],
   "source": [
    "output_width = 240\n",
    "output_height = 180\n",
    "def test():\n",
    "    test_loss = 0\n",
    "    iteration = 0\n",
    "    num_test_samples = 640\n",
    "    with torch.no_grad():\n",
    "        for idx, (test_inputs, test_labels) in enumerate(Generator(testing_file, 1)):\n",
    "            if idx >= num_test_samples:\n",
    "                break\n",
    "            else:\n",
    "                output = net(torch.from_numpy(test_inputs.transpose(0,3,1,2)).to(device))\n",
    "                loss = loss_fn(output.cpu(), torch.from_numpy(test_labels).cpu())*(distance(output.cpu().detach().numpy(), test_labels)/4)\n",
    "                output = output[0].reshape(output_height, output_width).cpu().detach().numpy()\n",
    "                test_labels = test_labels[0].reshape(output_height, output_width)\n",
    "                test_loss += loss\n",
    "\n",
    "                FileName = f'Data/Output/output-%04d.png' % iteration\n",
    "                \n",
    "                iteration += 1\n",
    "            \n",
    "                cv2.imwrite(FileName, np.float32(output) * 20000)\n",
    "                \n",
    "#                 fig = plt.figure(figsize = (32,24))\n",
    "#                 ax1 = fig.add_subplot(2, 2, 1)\n",
    "#                 ax1.set_title('Output')\n",
    "#                 ax2 = fig.add_subplot(2, 2, 2)\n",
    "#                 ax2.set_title('Label')\n",
    "\n",
    "#                 sns.heatmap(output,ax=ax1, square=True, cbar_kws={'shrink': .8})\n",
    "#                 sns.heatmap(test_labels, vmin=0, vmax=1, ax=ax2, square=True, cbar_kws={'shrink': .8})\n",
    "#                 plt.show()\n",
    "        \n",
    "        test_loss = test_loss/num_test_samples\n",
    "        print('Average test loss: ' ,test_loss.numpy())\n",
    "        print('Testing completed')\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
