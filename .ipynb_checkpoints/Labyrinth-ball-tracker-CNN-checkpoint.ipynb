{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabia\\anaconda3\\envs\\pt-labi\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\fabia\\anaconda3\\envs\\pt-labi\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\fabia\\anaconda3\\envs\\pt-labi\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\fabia\\anaconda3\\envs\\pt-labi\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\fabia\\anaconda3\\envs\\pt-labi\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\fabia\\anaconda3\\envs\\pt-labi\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'past'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7b0b9efa787d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensorboard\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pt-labi\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     raise ImportError('TensorBoard logging requires TensorBoard with Python summary writer installed. '\n\u001b[0;32m      5\u001b[0m                       'This should be available in 1.14 or above.')\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFileWriter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSummaryWriter\u001b[0m  \u001b[1;31m# noqa F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\pt-labi\\lib\\site-packages\\torch\\utils\\tensorboard\\writer.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevent_file_writer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEventFileWriter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_convert_np\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_np\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_embedding\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_mat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmake_sprite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmake_tsv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mappend_pbtxt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_onnx_graph\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_onnx_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pt-labi\\lib\\site-packages\\torch\\utils\\tensorboard\\_convert_np.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcaffe2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pt-labi\\lib\\site-packages\\caffe2\\python\\workspace.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltins\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'past'"
     ]
    }
   ],
   "source": [
    "# Import all the libraries needed\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the input of the CNN\n",
    "fig = plt.figure(figsize = (30,30))\n",
    "ax1 = fig.add_subplot(3, 3, 1)\n",
    "ax1.set_title('Input red channel')\n",
    "ax2 = fig.add_subplot(3, 3, 2)\n",
    "ax2.set_title('Input green channel')\n",
    "ax3 = fig.add_subplot(3, 3, 3)\n",
    "ax3.set_title('Label')\n",
    "\n",
    "x = np.load('Data/traindir/r_data/r_data-1900.npy')\n",
    "y = np.load('Data/traindir/labels/label-1900.npy')\n",
    "z = np.load('Data/traindir/g_data/g_data-1900.npy')\n",
    "x = Image.fromarray(x.astype(np.uint8))\n",
    "y = Image.fromarray(y.astype(np.uint8))\n",
    "z = Image.fromarray(z.astype(np.uint8))\n",
    "\n",
    "x = TF.to_tensor(x)\n",
    "y = TF.to_tensor(y)\n",
    "z = TF.to_tensor(z)\n",
    "\n",
    "\n",
    "sns.heatmap(x[0], cmap='gray', ax=ax1, square=True, cbar_kws={'shrink': .6})\n",
    "sns.heatmap(z[0], ax=ax2, square=True, cbar_kws={'shrink': .6})\n",
    "sns.heatmap(y[0], ax=ax3, square=True, cbar_kws={'shrink': .6})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = SummaryWriter('Tensorboard')\n",
    "# train_inputs, train_labels = next(iter(dataloaders['train']))\n",
    "# img_grid = torchvision.utils.make_grid(train_inputs)\n",
    "# matplotlib_imshow(img_grid, one_channel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_width = 240\n",
    "input_height = 180\n",
    "\n",
    "output_width = 120\n",
    "output_height = 90\n",
    "\n",
    "# Tell the dataloader where to search and where to put the arrays\n",
    "traindir = 'Data/traindir'\n",
    "train_channels = [f'{traindir}/r_data', f'{traindir}/g_data']\n",
    "\n",
    "valdir = 'Data/valdir'\n",
    "val_channels = [f'{valdir}/r_data', f'{valdir}/g_data']\n",
    "\n",
    "testdir = 'Data/testdir'\n",
    "test_channels = [f'{testdir}/r_data', f'{testdir}/g_data']\n",
    "\n",
    "train_inputs = []\n",
    "train_labels = []\n",
    "\n",
    "val_inputs = []\n",
    "val_labels = []\n",
    "\n",
    "test_inputs = []\n",
    "test_labels = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the saved .npy arrays from the preprocessing.py file into the above arrays,\n",
    "# split up the red and green channel again so that the axes match in size\n",
    "def load_npy_arrays(phase):\n",
    "    \n",
    "    if phase == \"train\":\n",
    "        phase_channels = train_channels\n",
    "        phase_inputs = train_inputs\n",
    "        phasedir = traindir\n",
    "        phase_labels = train_labels\n",
    "    \n",
    "    elif phase == \"val\":\n",
    "        phase_channels = val_channels\n",
    "        phase_inputs = val_inputs\n",
    "        phasedir = valdir\n",
    "        phase_labels = val_labels\n",
    "    \n",
    "    elif phase == \"test\":\n",
    "        phase_channels = test_channels\n",
    "        phase_inputs = test_inputs\n",
    "        phasedir = testdir\n",
    "        phase_labels = test_labels\n",
    "    \n",
    "    else:\n",
    "        print('Valueerror: please pass one of the three accepted strings(\"train\", \"val\", \"test\")')\n",
    "\n",
    "    for channel in phase_channels:\n",
    "        path_to_channel = channel\n",
    "        for file in sorted(os.listdir(channel)):\n",
    "            filename = file\n",
    "            if filename.endswith('.npy'):\n",
    "                loaded_array = np.load(f'{path_to_channel}/{filename}')\n",
    "                phase_inputs.append(loaded_array)\n",
    "                continue\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "    labelfolder = f'{phasedir}/labels'\n",
    "    for file in sorted(os.listdir(labelfolder)):\n",
    "        filename = file\n",
    "        if filename.endswith('.npy'):\n",
    "            loaded_array = np.load(f'{labelfolder}/{filename}')\n",
    "            phase_labels.append(loaded_array)\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "    print(np.shape(phase_labels))\n",
    "\n",
    "load_npy_arrays(\"train\")\n",
    "load_npy_arrays(\"val\")\n",
    "load_npy_arrays(\"test\")\n",
    "\n",
    "train_inputs = np.asarray(train_inputs)\n",
    "train_inputs = np.split(train_inputs, 2, axis=0)\n",
    "train_inputs = np.asarray(train_inputs).transpose(1,0,2,3)\n",
    "print(np.shape(train_inputs))\n",
    "\n",
    "val_inputs = np.asarray(val_inputs)\n",
    "val_inputs = np.split(val_inputs, 2, axis=0)\n",
    "val_inputs = np.asarray(val_inputs).transpose(1,0,2,3)\n",
    "print(np.shape(val_inputs))\n",
    "\n",
    "test_inputs = np.asarray(test_inputs)\n",
    "test_inputs = np.split(test_inputs, 2, axis=0)\n",
    "test_inputs = np.asarray(test_inputs).transpose(1,0,2,3)\n",
    "print(np.shape(test_inputs))\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "val_labels = np.asarray(val_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "\n",
    "# Define our own dataset creator because we want to load numpy arrays,\n",
    "# but still be able to apply transformations as if it were images\n",
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = torch.FloatTensor(inputs)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.inputs[index]\n",
    "        y = self.labels[index]\n",
    "        \n",
    "        xred = x[0]\n",
    "        xgreen = x[1]\n",
    "        \n",
    "        xred = Image.fromarray(xred.numpy().astype(np.uint8))\n",
    "        xgreen = Image.fromarray(xgreen.numpy().astype(np.uint8))\n",
    "        y = Image.fromarray(self.labels[index].numpy().astype(np.uint8))\n",
    "        \n",
    "        if random.random() > 0.5:\n",
    "            xred = TF.hflip(xred)\n",
    "            xgreen = TF.hflip(xgreen)\n",
    "            y = TF.hflip(y)\n",
    "            \n",
    "        if random.random() > 0.5:\n",
    "            xred = TF.vflip(xred)\n",
    "            xgreen = TF.vflip(xgreen)\n",
    "            y = TF.vflip(y)\n",
    "            \n",
    "        if random.random() > 0.5: \n",
    "            angle = random.randint(-180, 180)\n",
    "            \n",
    "            xred = TF.resize(xred, (input_height*2,input_width*2), interpolation=2)\n",
    "            xgreen = TF.resize(xgreen, (input_height*2,input_width*2), interpolation=2)\n",
    "            y = TF.resize(y, (output_height*2,output_width*2), interpolation=2)\n",
    "            \n",
    "            filler = 0.0 if xred.mode.startswith(\"F\") else 0\n",
    "            num_bands = len(xred.getbands())\n",
    "            xred = TF.rotate(xred, angle)\n",
    "            \n",
    "            filler = 0.0 if xgreen.mode.startswith(\"F\") else 0\n",
    "            num_bands = len(xgreen.getbands())\n",
    "            xgreen = TF.rotate(xgreen, angle)\n",
    "            \n",
    "            filler = 0.0 if y.mode.startswith(\"F\") else 0\n",
    "            num_bands = len(y.getbands())\n",
    "            y = TF.rotate(y, angle)\n",
    "            \n",
    "            xred = TF.resize(xred, (input_height,input_width), interpolation=2)\n",
    "            xgreen = TF.resize(xgreen, (input_height,input_width), interpolation=2)\n",
    "            y = TF.resize(y, (output_height,output_width), interpolation=2)\n",
    "        \n",
    "        xred = TF.to_tensor(xred)\n",
    "        xgreen = TF.to_tensor(xgreen)\n",
    "        y = TF.to_tensor(y)\n",
    "        \n",
    "        x[0] = xred\n",
    "        x[1] = xgreen\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    \n",
    "# Get the data, transform it\n",
    "data = {\n",
    "   'train':\n",
    "   CreateDataset(train_inputs, train_labels),\n",
    "   'val':\n",
    "   CreateDataset(val_inputs, val_labels),\n",
    "    'test':\n",
    "   CreateDataset(test_inputs, test_labels),\n",
    "}\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "print('Sum of all labels in a batch: ' ,batch_size*np.sum(train_labels[0]/255))\n",
    "\n",
    "# Load Data in batches, shuffled\n",
    "dataloaders = {\n",
    "   'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True, drop_last=True),\n",
    "   'val': DataLoader(data['val'], batch_size=batch_size, shuffle=True, drop_last=True),\n",
    "    'test': DataLoader(data['test'], batch_size=batch_size, shuffle=True, drop_last=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in pretrained VGG16 net, extract the FCN part, delete pooling layers,\n",
    "# modify first layer to recieve 2 channels, the last one to output depth 1\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(2,16,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Sigmoid())\n",
    "    \n",
    "        self.layer2 = nn.Sequential( \n",
    "            nn.Conv2d(16,32,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool3d((1,2,2), padding=0),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32,64,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),  \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64,1,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),  \n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)  \n",
    "        x = self.layer2(x)  \n",
    "        x = self.layer3(x)  \n",
    "        x = self.layer4(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available and move the model over to GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda: 0\")\n",
    "    gpu_name = torch.cuda.get_device_name()\n",
    "    print(f\"Running on your {gpu_name} (GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on your CPU\")\n",
    "\n",
    "net = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the training loop with loss-function and optimizer\n",
    "\n",
    "loss_fn = nn.L1Loss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 15\n",
    "epochs_no_improve_limit = 7\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "def train():\n",
    "    epochs_no_improve = 0\n",
    "    min_val_loss = np.Inf\n",
    "    since = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, epochs))\n",
    "        print('-' * 10)\n",
    "        val_loss = 0\n",
    "        \n",
    "        for train_inputs, train_labels in dataloaders['train']:\n",
    "            optimizer.zero_grad()\n",
    "            output = net(train_inputs.to(device)).reshape(batch_size, output_height, output_width)\n",
    "            loss = loss_fn(output.cpu(), train_labels.cpu().reshape(batch_size, output_height, output_width))\n",
    "            \n",
    "#             fig = plt.figure(figsize = (32,24))\n",
    "#             ax1 = fig.add_subplot(2, 2, 1)\n",
    "#             ax1.set_title('Input red')\n",
    "#             ax2 = fig.add_subplot(2, 2, 2)\n",
    "#             ax2.set_title('Input green')\n",
    "#             ax3 = fig.add_subplot(2, 2, 3)\n",
    "#             ax3.set_title('Label')\n",
    "#             ax4 = fig.add_subplot(2, 2, 4)\n",
    "#             ax4.set_title('Output of the CNN')\n",
    "\n",
    "#             sns.heatmap(train_inputs[0][0], cmap='gray', ax=ax1, cbar_kws={'shrink': .6})\n",
    "#             sns.heatmap(train_inputs[0][1], ax=ax2, cbar_kws={'shrink': .6})\n",
    "#             sns.heatmap(train_labels[0][0], ax=ax3, cbar_kws={'shrink': .6})\n",
    "#             sns.heatmap(output.cpu().detach().numpy()[0], ax=ax4, cbar_kws={'shrink': .6})\n",
    "#             plt.show()\n",
    "            \n",
    "            train_losses.append(float(loss))\n",
    "            print('Training Loss: {:.4f}'.format(loss))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        del train_inputs\n",
    "        del train_labels\n",
    "        del output\n",
    "        del loss\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in dataloaders['val']:\n",
    "                torch.cuda.empty_cache()\n",
    "                output = net(val_inputs.to(device)).reshape(batch_size, output_height, output_width)\n",
    "                loss = loss_fn(output.cpu(), val_labels.cpu().reshape(batch_size, output_height, output_width))\n",
    "                val_loss += loss\n",
    "\n",
    "            val_loss = val_loss / len(dataloaders['val'])\n",
    "            val_losses.append(float(val_loss))\n",
    "            print('-' * 10)\n",
    "            print('Validation Loss: {:.4f}'.format(val_loss))\n",
    "\n",
    "            if val_loss < min_val_loss:\n",
    "                torch.save({'state_dict': net.state_dict()}, 'Nets/pt-labi_CNN_minloss{:.4f}.pt'.format(min_val_loss))\n",
    "                epochs_no_improve = 0\n",
    "                min_val_loss = val_loss\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve == epochs_no_improve_limit:\n",
    "                    print('Early stopping initiated')\n",
    "                    model = torch.load('Nets/pt-labi_CNN_minloss{:.4f}.pt'.format(min_val_loss))\n",
    "                    print('Best model so far has been loaded')\n",
    "    print('Least validation Loss: {:4f}'.format(min_val_loss))\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Finished training')\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the testing loop and output some heatmaps\n",
    "# to estimate the performance of the CNN\n",
    "\n",
    "def test():\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in dataloaders['test']:\n",
    "            output = net(test_inputs.to(device)).reshape(batch_size, output_height, output_width)\n",
    "            loss = loss_fn(output.cpu(), test_labels[0].cpu())\n",
    "            output = output[0].reshape(output_height, output_width).cpu().detach().numpy()\n",
    "            test_labels = test_labels[0].reshape(output_height, output_width)\n",
    "            test_loss += loss\n",
    "\n",
    "            fig = plt.figure(figsize = (32,24))\n",
    "            ax1 = fig.add_subplot(2, 2, 1)\n",
    "            ax1.set_title('Output')\n",
    "            ax2 = fig.add_subplot(2, 2, 2)\n",
    "            ax2.set_title('Label')\n",
    "\n",
    "            sns.heatmap(output, ax=ax1, square=True, cbar_kws={'shrink': .6})\n",
    "            sns.heatmap(test_labels, ax=ax2, square=True, cbar_kws={'shrink': .6})\n",
    "            plt.show()\n",
    "        \n",
    "        test_loss = test_loss/len(dataloaders['test'])\n",
    "        print('Average test loss: ' ,test_loss.numpy())\n",
    "        print('Testing completed')\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
